"""
ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚·ã‚¹ãƒ†ãƒ ç”¨ãƒ¢ãƒ‡ãƒ«è¨“ç·´ãƒ»ä¿å­˜ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

Phase 1.8ï¼ˆ93.64%æ–¹å‘äºˆæ¸¬ï¼‰ã¨Phase 2ï¼ˆSharpe 4.07åŽç›Šäºˆæ¸¬ï¼‰ã®
ä¸¡æ–¹ã‚’è¨“ç·´ã—ã€ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚·ã‚¹ãƒ†ãƒ ã§ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ä¿å­˜
"""

from pathlib import Path
from datetime import datetime, timedelta
from loguru import logger
import pandas as pd
import numpy as np
import sys
import joblib
from typing import Dict, Tuple

from src.data_sources.yahoo_finance import YahooFinanceData
from src.data_sources.economic_indicators import EconomicIndicators
from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor, RandomForestClassifier, RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import xgboost as xgb
import lightgbm as lgb
from catboost import CatBoostClassifier, CatBoostRegressor


class HybridModelTrainer:
    """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚·ã‚¹ãƒ†ãƒ ç”¨ãƒ¢ãƒ‡ãƒ«è¨“ç·´"""

    def __init__(self):
        self.yahoo = YahooFinanceData()
        self.fred = EconomicIndicators()

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé€šè²¨ãƒšã‚¢
        self.instrument = 'USD/JPY'

        # ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.phase1_8_dir = Path('models/phase1_8')
        self.phase2_dir = Path('models/phase2')

        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        self.phase1_8_dir.mkdir(parents=True, exist_ok=True)
        self.phase2_dir.mkdir(parents=True, exist_ok=True)

        logger.info("="*80)
        logger.info("ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚·ã‚¹ãƒ†ãƒ ç”¨ãƒ¢ãƒ‡ãƒ«è¨“ç·´ãƒ»ä¿å­˜")
        logger.info("="*80)
        logger.info(f"Phase 1.8ä¿å­˜å…ˆ: {self.phase1_8_dir}")
        logger.info(f"Phase 2ä¿å­˜å…ˆ: {self.phase2_dir}")

    def run(self):
        """å®Ÿè¡Œ"""
        try:
            # 1. ãƒ‡ãƒ¼ã‚¿å–å¾—
            logger.info("\n1. ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­ï¼ˆ10å¹´åˆ†ï¼‰...")
            data = self._get_data()

            if data.empty:
                logger.error("ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—")
                return False

            logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿: {len(data)}æ—¥åˆ†")

            # 2. ç‰¹å¾´é‡ç”Ÿæˆ
            logger.info("\n2. ç‰¹å¾´é‡ç”Ÿæˆä¸­...")
            features_all, features_phase1 = self._generate_features(data)
            logger.info(f"âœ… ç‰¹å¾´é‡ï¼ˆå…¨ä½“ï¼‰: {features_all.shape}")
            logger.info(f"   ç‰¹å¾´é‡ï¼ˆPhase 1.8ç”¨ï¼‰: {features_phase1.shape}")

            # 3. Phase 1.8è¨“ç·´ãƒ»ä¿å­˜
            logger.info("\n3. Phase 1.8ãƒ¢ãƒ‡ãƒ«è¨“ç·´ãƒ»ä¿å­˜ï¼ˆç›®æ¨™93.64%ï¼‰...")
            phase1_results = self._train_and_save_phase1((features_all, features_phase1))

            # 4. Phase 2è¨“ç·´ãƒ»ä¿å­˜
            logger.info("\n4. Phase 2ãƒ¢ãƒ‡ãƒ«è¨“ç·´ãƒ»ä¿å­˜ï¼ˆç›®æ¨™Sharpe 4+ï¼‰...")
            phase2_results = self._train_and_save_phase2((features_all, features_phase1))

            # 5. çµæžœã‚µãƒžãƒªãƒ¼
            logger.info("\n5. è¨“ç·´çµæžœã‚µãƒžãƒªãƒ¼")
            self._print_summary(phase1_results, phase2_results)

            return True

        except Exception as e:
            logger.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return False

    def _get_data(self):
        """ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆ10å¹´åˆ†ï¼‰"""
        end_date = datetime.now()
        start_date = end_date - timedelta(days=3650)  # 10å¹´

        data = self.yahoo.get_forex_data(
            pair='USD/JPY',
            start_date=start_date.strftime('%Y-%m-%d'),
            end_date=end_date.strftime('%Y-%m-%d')
        )

        return data

    def _generate_features(self, data):
        """ç‰¹å¾´é‡ç”Ÿæˆï¼ˆPhase 1.8 + Phase 2å…±é€šï¼‰"""
        df = data.copy()
        features = pd.DataFrame(index=df.index)

        # åŸºæœ¬ä¾¡æ ¼
        features['close'] = df['close']
        features['open'] = df['open']
        features['high'] = df['high']
        features['low'] = df['low']
        features['volume'] = df['volume']

        # ä¾¡æ ¼æ¯”çŽ‡
        features['high_close_ratio'] = (df['high'] / df['close'] - 1) * 100
        features['low_close_ratio'] = (df['low'] / df['close'] - 1) * 100
        features['high_low_range'] = (df['high'] / df['low'] - 1) * 100

        # ãƒªã‚¿ãƒ¼ãƒ³
        for period in [1, 5, 10, 20]:
            features[f'return_{period}d'] = df['close'].pct_change(period) * 100

        # SMA
        for period in [5, 10, 20, 50, 100, 200]:
            sma = df['close'].rolling(period).mean()
            features[f'sma_{period}'] = sma
            features[f'price_vs_sma_{period}'] = ((df['close'] / sma) - 1) * 100

        # EMA
        for period in [12, 26]:
            ema = df['close'].ewm(span=period).mean()
            features[f'ema_{period}'] = ema

        # RSI
        for period in [7, 14, 21]:
            delta = df['close'].diff()
            gain = delta.where(delta > 0, 0).rolling(period).mean()
            loss = -delta.where(delta < 0, 0).rolling(period).mean()
            rs = gain / loss
            features[f'rsi_{period}'] = 100 - (100 / (1 + rs))

        # MACD
        ema12 = df['close'].ewm(span=12).mean()
        ema26 = df['close'].ewm(span=26).mean()
        features['macd'] = ema12 - ema26

        # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
        for period in [10, 20, 50]:
            features[f'volatility_{period}d'] = df['close'].pct_change().rolling(period).std() * 100

        # ATR
        high_low = df['high'] - df['low']
        high_close = abs(df['high'] - df['close'].shift(1))
        low_close = abs(df['low'] - df['close'].shift(1))
        true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)
        features['atr_14'] = true_range.rolling(14).mean()

        # ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ãƒãƒ³ãƒ‰
        sma_20 = df['close'].rolling(20).mean()
        std_20 = df['close'].rolling(20).std()
        features['bb_upper'] = sma_20 + (std_20 * 2)
        features['bb_lower'] = sma_20 - (std_20 * 2)
        features['bb_position'] = ((df['close'] - features['bb_lower']) /
                                   (features['bb_upper'] - features['bb_lower']) * 100)

        # ã‚¹ãƒˆã‚­ãƒ£ã‚¹ãƒ†ã‚£ã‚¯ã‚¹
        for period in [14, 21]:
            lowest_low = df['low'].rolling(period).min()
            highest_high = df['high'].rolling(period).max()
            features[f'stoch_{period}'] = ((df['close'] - lowest_low) /
                                          (highest_high - lowest_low) * 100)

        # ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ 
        for period in [10, 20]:
            features[f'momentum_{period}'] = df['close'] - df['close'].shift(period)

        # ROC
        for period in [10, 20]:
            features[f'roc_{period}'] = ((df['close'] - df['close'].shift(period)) /
                                        df['close'].shift(period) * 100)

        # ãƒ©ãƒ™ãƒ«ï¼ˆPhase 1.8ç”¨: é–¾å€¤ãƒ™ãƒ¼ã‚¹æ–¹å‘ï¼‰
        threshold = 0.5  # Â±0.5%
        price_change = ((df['close'].shift(-1) / df['close']) - 1) * 100

        features['label_direction_raw'] = np.where(
            price_change > threshold, 1,
            np.where(price_change < -threshold, 0, -1)
        )

        # ãƒ©ãƒ™ãƒ«ï¼ˆPhase 2ç”¨: å®Ÿéš›ã®ãƒªã‚¿ãƒ¼ãƒ³ï¼‰
        features['label_return'] = price_change

        # NaNé™¤åŽ»
        features = features.dropna()

        # Phase 1.8ç”¨: ä¸­ç«‹ã‚’é™¤å¤–
        features_phase1 = features[features['label_direction_raw'] != -1].copy()
        features_phase1['label_direction'] = features_phase1['label_direction_raw']

        return features, features_phase1

    def _train_and_save_phase1(self, data_tuple):
        """Phase 1.8è¨“ç·´ãƒ»ä¿å­˜"""
        features_all, features_phase1 = data_tuple

        logger.info(f"  ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(features_phase1)}ä»¶ï¼ˆÂ±0.5%ä»¥ä¸Šã®ã¿ï¼‰")

        # ç‰¹å¾´é‡ã¨ãƒ©ãƒ™ãƒ«
        feature_cols = [col for col in features_phase1.columns
                       if not col.startswith('label_')]

        X = features_phase1[feature_cols].values
        y = features_phase1['label_direction'].values

        # åˆ†å‰²ï¼ˆ70/15/15ï¼‰
        X_temp, X_test, y_temp, y_test = train_test_split(
            X, y, test_size=0.15, shuffle=False
        )
        X_train, X_val, y_train, y_val = train_test_split(
            X_temp, y_temp, test_size=0.176, shuffle=False  # 0.15/0.85 â‰ˆ 0.176
        )

        logger.info(f"  è¨“ç·´: {len(X_train)}, æ¤œè¨¼: {len(X_val)}, ãƒ†ã‚¹ãƒˆ: {len(X_test)}")

        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_val_scaled = scaler.transform(X_val)
        X_test_scaled = scaler.transform(X_test)

        # æ™‚é–“é‡ã¿
        sample_weights = np.array([0.95 ** i for i in range(len(X_train))])[::-1]

        # 5ãƒ¢ãƒ‡ãƒ«è¨“ç·´
        models = {}
        val_accuracies = {}

        # 1. GradientBoosting
        logger.info("  è¨“ç·´ä¸­: GradientBoosting...")
        gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.05, max_depth=5, random_state=42)
        gb.fit(X_train_scaled, y_train, sample_weight=sample_weights)
        val_acc = (gb.predict(X_val_scaled) == y_val).mean()
        models['gradient_boosting'] = gb
        val_accuracies['gradient_boosting'] = val_acc

        # 2. RandomForest
        logger.info("  è¨“ç·´ä¸­: RandomForest...")
        rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)
        rf.fit(X_train_scaled, y_train, sample_weight=sample_weights)
        val_acc = (rf.predict(X_val_scaled) == y_val).mean()
        models['random_forest'] = rf
        val_accuracies['random_forest'] = val_acc

        # 3. XGBoost
        logger.info("  è¨“ç·´ä¸­: XGBoost...")
        xgb_model = xgb.XGBClassifier(n_estimators=100, learning_rate=0.05, max_depth=5, random_state=42)
        xgb_model.fit(X_train_scaled, y_train, sample_weight=sample_weights)
        val_acc = (xgb_model.predict(X_val_scaled) == y_val).mean()
        models['xgboost'] = xgb_model
        val_accuracies['xgboost'] = val_acc

        # 4. LightGBM
        logger.info("  è¨“ç·´ä¸­: LightGBM...")
        lgb_model = lgb.LGBMClassifier(n_estimators=100, learning_rate=0.05, max_depth=5, random_state=42, verbose=-1)
        lgb_model.fit(X_train_scaled, y_train, sample_weight=sample_weights)
        val_acc = (lgb_model.predict(X_val_scaled) == y_val).mean()
        models['lightgbm'] = lgb_model
        val_accuracies['lightgbm'] = val_acc

        # 5. CatBoost
        logger.info("  è¨“ç·´ä¸­: CatBoost...")
        cb = CatBoostClassifier(iterations=100, learning_rate=0.05, depth=5, random_state=42, verbose=0)
        cb.fit(X_train_scaled, y_train, sample_weight=sample_weights)
        val_acc = (cb.predict(X_val_scaled) == y_val).mean()
        models['catboost'] = cb
        val_accuracies['catboost'] = val_acc

        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é‡ã¿ï¼ˆæ¤œè¨¼ç²¾åº¦ãƒ™ãƒ¼ã‚¹ï¼‰
        total_acc = sum(val_accuracies.values())
        weights = {name: acc/total_acc for name, acc in val_accuracies.items()}

        logger.info("\n  ãƒ¢ãƒ‡ãƒ«é‡ã¿:")
        for name, weight in weights.items():
            logger.info(f"    {name}: {weight:.4f} (æ¤œè¨¼ç²¾åº¦: {val_accuracies[name]*100:.2f}%)")

        # ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆäºˆæ¸¬ï¼ˆä¿¡é ¼åº¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼‰
        ensemble_probs = np.zeros((len(X_test_scaled), 2))
        for name, model in models.items():
            probs = model.predict_proba(X_test_scaled)
            ensemble_probs += probs * weights[name]

        # ä¿¡é ¼åº¦ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆ0.65ä»¥ä¸Š or 0.35ä»¥ä¸‹ï¼‰
        confidence_threshold = 0.65
        max_probs = ensemble_probs.max(axis=1)
        confident_mask = max_probs >= confidence_threshold

        predictions = ensemble_probs.argmax(axis=1)
        confident_predictions = predictions[confident_mask]
        confident_labels = y_test[confident_mask]

        accuracy = (confident_predictions == confident_labels).mean() * 100
        coverage = confident_mask.sum() / len(y_test) * 100

        logger.info(f"\nâœ… Phase 1.8çµæžœ:")
        logger.info(f"  æ–¹å‘æ€§çš„ä¸­çŽ‡: {accuracy:.2f}%")
        logger.info(f"  ã‚«ãƒãƒ¼çŽ‡: {coverage:.2f}%")
        logger.info(f"  è¦‹é€ã‚Š: {(~confident_mask).sum()}ä»¶")

        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        logger.info(f"\n  ãƒ¢ãƒ‡ãƒ«ä¿å­˜ä¸­: {self.phase1_8_dir}")

        save_data = {
            'models': models,
            'weights': weights,
            'scaler': scaler,
            'feature_columns': feature_cols,
            'confidence_threshold': confidence_threshold,
            'metadata': {
                'accuracy': accuracy,
                'coverage': coverage,
                'train_samples': len(X_train),
                'val_accuracies': val_accuracies
            }
        }

        # é€šè²¨ãƒšã‚¢åã‚’å«ã‚€ãƒ•ã‚¡ã‚¤ãƒ«åã§ä¿å­˜
        pair_code = self.instrument.replace('/', '_')
        model_filename = f'{pair_code}_ensemble_models.pkl'
        joblib.dump(save_data, self.phase1_8_dir / model_filename)
        logger.info(f"  âœ… Phase 1.8ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: {model_filename}")

        return {
            'accuracy': accuracy,
            'coverage': coverage,
            'models': models,
            'weights': weights
        }

    def _train_and_save_phase2(self, data_tuple):
        """Phase 2è¨“ç·´ãƒ»ä¿å­˜"""
        features_all, _ = data_tuple

        logger.info(f"  ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(features_all)}ä»¶ï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿ï¼‰")

        # ç‰¹å¾´é‡ã¨ãƒ©ãƒ™ãƒ«
        feature_cols = [col for col in features_all.columns
                       if not col.startswith('label_')]

        X = features_all[feature_cols].values
        y = features_all['label_return'].values

        # åˆ†å‰²ï¼ˆ70/15/15ï¼‰
        X_temp, X_test, y_temp, y_test = train_test_split(
            X, y, test_size=0.15, shuffle=False
        )
        X_train, X_val, y_train, y_val = train_test_split(
            X_temp, y_temp, test_size=0.176, shuffle=False
        )

        logger.info(f"  è¨“ç·´: {len(X_train)}, æ¤œè¨¼: {len(X_val)}, ãƒ†ã‚¹ãƒˆ: {len(X_test)}")

        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        # XGBoostè¨“ç·´ï¼ˆPhase 2ã®ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ï¼‰
        logger.info("  è¨“ç·´ä¸­: XGBoostï¼ˆå›žå¸°ï¼‰...")
        model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.05, max_depth=5, random_state=42)
        model.fit(X_train_scaled, y_train)

        # äºˆæ¸¬
        y_pred = model.predict(X_test_scaled)

        # Phase 2è©•ä¾¡
        positions = np.sign(y_pred)
        returns = positions * y_test

        sharpe = (returns.mean() / returns.std() * np.sqrt(252)) if returns.std() > 0 else 0
        cumulative_return = returns.sum()
        win_rate = (returns > 0).sum() / len(returns) * 100

        wins = returns[returns > 0]
        losses = returns[returns <= 0]
        profit_factor = wins.sum() / abs(losses.sum()) if losses.sum() != 0 else 0

        logger.info(f"\nâœ… Phase 2çµæžœ:")
        logger.info(f"  Sharpe Ratio: {sharpe:.2f}")
        logger.info(f"  ç´¯ç©ãƒªã‚¿ãƒ¼ãƒ³: {cumulative_return:.2f}%")
        logger.info(f"  å‹çŽ‡: {win_rate:.2f}%")
        logger.info(f"  PF: {profit_factor:.2f}")

        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        logger.info(f"\n  ãƒ¢ãƒ‡ãƒ«ä¿å­˜ä¸­: {self.phase2_dir}")

        save_data = {
            'model': model,
            'scaler': scaler,
            'feature_columns': feature_cols,
            'metadata': {
                'sharpe': sharpe,
                'cumulative_return': cumulative_return,
                'win_rate': win_rate,
                'profit_factor': profit_factor
            }
        }

        # é€šè²¨ãƒšã‚¢åã‚’å«ã‚€ãƒ•ã‚¡ã‚¤ãƒ«åã§ä¿å­˜
        pair_code = self.instrument.replace('/', '_')
        model_filename = f'{pair_code}_xgboost_model.pkl'
        joblib.dump(save_data, self.phase2_dir / model_filename)
        logger.info(f"  âœ… Phase 2ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: {model_filename}")

        return {
            'sharpe': sharpe,
            'cumulative_return': cumulative_return,
            'win_rate': win_rate,
            'profit_factor': profit_factor
        }

    def _print_summary(self, phase1, phase2):
        """çµæžœã‚µãƒžãƒªãƒ¼"""
        logger.info("\n" + "="*80)
        logger.info("è¨“ç·´å®Œäº†ã‚µãƒžãƒªãƒ¼")
        logger.info("="*80)

        logger.info("\nPhase 1.8ï¼ˆæ–¹å‘äºˆæ¸¬ï¼‰:")
        logger.info(f"  âœ… ä¿å­˜å…ˆ: {self.phase1_8_dir}/ensemble_models.pkl")
        logger.info(f"  æ–¹å‘æ€§çš„ä¸­çŽ‡: {phase1['accuracy']:.2f}%")
        logger.info(f"  ã‚«ãƒãƒ¼çŽ‡: {phase1['coverage']:.2f}%")

        logger.info("\nPhase 2ï¼ˆåŽç›Šäºˆæ¸¬ï¼‰:")
        logger.info(f"  âœ… ä¿å­˜å…ˆ: {self.phase2_dir}/xgboost_model.pkl")
        logger.info(f"  Sharpe Ratio: {phase2['sharpe']:.2f}")
        logger.info(f"  å‹çŽ‡: {phase2['win_rate']:.2f}%")

        logger.info("\næ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
        logger.info("  1. ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰äºˆæ¸¬ã‚¨ãƒ³ã‚¸ãƒ³æ§‹ç¯‰")
        logger.info("  2. ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Ÿæ–½")
        logger.info("  3. æ€§èƒ½è©•ä¾¡")

        logger.info("="*80)


if __name__ == "__main__":
    try:
        trainer = HybridModelTrainer()
        success = trainer.run()

        if success:
            logger.success("\nðŸš€ ãƒ¢ãƒ‡ãƒ«è¨“ç·´ãƒ»ä¿å­˜å®Œäº†ï¼")
            logger.info("ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚·ã‚¹ãƒ†ãƒ ã®æº–å‚™ãŒæ•´ã„ã¾ã—ãŸ")
            sys.exit(0)
        else:
            logger.error("\nâŒ è¨“ç·´å¤±æ•—")
            sys.exit(1)

    except Exception as e:
        logger.error(f"\nã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        logger.error(traceback.format_exc())
        sys.exit(1)
