"""
æ™‚é–“è¶³ãƒ¢ãƒ‡ãƒ«è¨“ç·´ - Phase 1.8 & Phase 2
ã™ã¹ã¦ã®ä¸Šä¸‹ç§»å‹•ã‚’æ‰ãˆã‚‹ä¸–ç•Œæœ€å¼·ã‚·ã‚¹ãƒ†ãƒ 
"""
from pathlib import Path
from datetime import datetime
from loguru import logger
import pandas as pd
import numpy as np
import joblib
from typing import Tuple

from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, GradientBoostingRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from xgboost import XGBClassifier, XGBRegressor
from lightgbm import LGBMClassifier, LGBMRegressor
from catboost import CatBoostClassifier, CatBoostRegressor

from src.data_sources.yahoo_finance_hourly import YahooFinanceHourly


class HourlyModelTrainer:
    """æ™‚é–“è¶³ãƒ‡ãƒ¼ã‚¿ã§Phase 1.8ã¨Phase 2ã‚’è¨“ç·´"""

    def __init__(self, pair: str = 'USDJPY=X'):
        self.pair = pair
        self.hourly_data = YahooFinanceHourly(interval='1h')

        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.models_dir = Path('models/hourly')
        self.models_dir.mkdir(parents=True, exist_ok=True)

        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.output_dir = Path('outputs/hourly_training')
        self.output_dir.mkdir(parents=True, exist_ok=True)

        logger.info(f"æ™‚é–“è¶³ãƒ¢ãƒ‡ãƒ«è¨“ç·´å™¨ åˆæœŸåŒ–: {pair}")

    def load_and_prepare_data(self) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨ç‰¹å¾´é‡ç”Ÿæˆ"""
        logger.info("\n" + "="*80)
        logger.info("1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿")
        logger.info("="*80)

        # ä¿å­˜æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        df = self.hourly_data.load_saved_data(pair=self.pair, period='2y')

        if df is None:
            raise ValueError(f"ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.pair}")

        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’DatetimeIndexã«å¤‰æ›ï¼ˆUTCå¯¾å¿œï¼‰
        df.index = pd.to_datetime(df.index, utc=True)

        logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df)}æœ¬")

        # ç‰¹å¾´é‡ç”Ÿæˆ
        logger.info("\n2. ç‰¹å¾´é‡ç”Ÿæˆä¸­...")
        features_all = self._generate_features(df)
        features_phase1 = features_all.copy()  # Phase 1.8ç”¨

        logger.info(f"âœ… ç‰¹å¾´é‡ç”Ÿæˆå®Œäº†")
        logger.info(f"  å…¨ç‰¹å¾´é‡: {features_all.shape}")
        logger.info(f"  Phase 1.8ç”¨: {features_phase1.shape}")

        return features_all, features_phase1

    def _generate_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ™‚é–“è¶³ã«æœ€é©åŒ–ã•ã‚ŒãŸç‰¹å¾´é‡ç”Ÿæˆ"""
        features = pd.DataFrame(index=df.index)

        # åŸºæœ¬ä¾¡æ ¼ç‰¹å¾´é‡
        features['price'] = df['close']
        features['return_1h'] = df['close'].pct_change(1) * 100
        features['return_4h'] = df['close'].pct_change(4) * 100
        features['return_24h'] = df['close'].pct_change(24) * 100

        # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
        features['volatility_24h'] = df['close'].pct_change().rolling(24).std() * 100
        features['volatility_168h'] = df['close'].pct_change().rolling(168).std() * 100  # 1é€±é–“

        # ç§»å‹•å¹³å‡
        for period in [12, 24, 48, 168]:  # 12h, 1d, 2d, 1week
            features[f'sma_{period}'] = df['close'].rolling(period).mean()
            features[f'price_to_sma_{period}'] = (df['close'] / features[f'sma_{period}'] - 1) * 100

        # ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™
        # RSI
        for period in [14, 28]:
            delta = df['close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(period).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(period).mean()
            rs = gain / loss
            features[f'rsi_{period}'] = 100 - (100 / (1 + rs))

        # MACD
        ema12 = df['close'].ewm(span=12, adjust=False).mean()
        ema26 = df['close'].ewm(span=26, adjust=False).mean()
        features['macd'] = ema12 - ema26
        features['macd_signal'] = features['macd'].ewm(span=9, adjust=False).mean()
        features['macd_diff'] = features['macd'] - features['macd_signal']

        # ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ãƒãƒ³ãƒ‰
        for period in [20, 40]:
            sma = df['close'].rolling(period).mean()
            std = df['close'].rolling(period).std()
            features[f'bb_upper_{period}'] = sma + (std * 2)
            features[f'bb_lower_{period}'] = sma - (std * 2)
            features[f'bb_width_{period}'] = (features[f'bb_upper_{period}'] - features[f'bb_lower_{period}']) / sma * 100

        # æ™‚é–“å¸¯ç‰¹å¾´ï¼ˆFXå¸‚å ´ã®æ™‚é–“å¸¯ï¼‰
        features['hour'] = df.index.hour
        features['is_asian_session'] = ((features['hour'] >= 0) & (features['hour'] < 9)).astype(int)
        features['is_european_session'] = ((features['hour'] >= 8) & (features['hour'] < 17)).astype(int)
        features['is_us_session'] = ((features['hour'] >= 13) & (features['hour'] < 22)).astype(int)

        # æ›œæ—¥
        features['day_of_week'] = df.index.dayofweek

        logger.info(f"  ç”Ÿæˆã•ã‚ŒãŸç‰¹å¾´é‡æ•°: {len([c for c in features.columns if c not in ['label_direction', 'label_direction_raw', 'actual_return']])}")

        return features

    def train_phase1_8(self, features_df: pd.DataFrame) -> dict:
        """Phase 1.8: æ–¹å‘äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«è¨“ç·´ï¼ˆæ™‚é–“è¶³ç‰ˆï¼‰"""
        logger.info("\n" + "="*80)
        logger.info("Phase 1.8 è¨“ç·´é–‹å§‹ï¼ˆæ–¹å‘äºˆæ¸¬ï¼‰")
        logger.info("="*80)

        # ãƒ©ãƒ™ãƒ«ç”Ÿæˆï¼ˆæ™‚é–“è¶³ã§ã¯Â±0.3%ï¼‰
        df = self.hourly_data.load_saved_data(pair=self.pair, period='2y')
        df.index = pd.to_datetime(df.index, utc=True)
        threshold = 0.003  # 0.3%ï¼ˆæ—¥æ¬¡ã®0.5%ã‚ˆã‚Šå°ã•ãï¼‰

        price_change = ((df['close'].shift(-1) / df['close']) - 1) * 100
        features_df['label_direction_raw'] = np.where(
            price_change > (threshold * 100), 1,
            np.where(price_change < -(threshold * 100), 0, -1)
        )

        # ä¸­ç«‹ãƒ©ãƒ™ãƒ«ã‚’é™¤å¤–
        features_df = features_df[features_df['label_direction_raw'] != -1].copy()
        features_df['label_direction'] = features_df['label_direction_raw']

        logger.info(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(features_df)}ã‚µãƒ³ãƒ—ãƒ«")
        logger.info(f"  ä¸Šæ˜‡: {(features_df['label_direction'] == 1).sum()}")
        logger.info(f"  ä¸‹é™: {(features_df['label_direction'] == 0).sum()}")

        # NaNé™¤å»
        feature_cols = [c for c in features_df.columns if c not in ['label_direction', 'label_direction_raw', 'actual_return']]
        features_df = features_df.dropna(subset=feature_cols + ['label_direction'])

        X = features_df[feature_cols].values
        y = features_df['label_direction'].values

        # è¨“ç·´/æ¤œè¨¼/ãƒ†ã‚¹ãƒˆåˆ†å‰²
        X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.15, shuffle=False)
        X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.176, shuffle=False)

        logger.info(f"è¨“ç·´: {len(X_train)}, æ¤œè¨¼: {len(X_val)}, ãƒ†ã‚¹ãƒˆ: {len(X_test)}")

        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_val_scaled = scaler.transform(X_val)
        X_test_scaled = scaler.transform(X_test)

        # æ™‚é–“é‡ã¿ä»˜ã‘
        decay_rate = 0.95
        sample_weights = np.array([decay_rate ** i for i in range(len(X_train))])[::-1]

        # 5ç¨®é¡ã®ãƒ¢ãƒ‡ãƒ«è¨“ç·´
        models = {}
        logger.info("\nãƒ¢ãƒ‡ãƒ«è¨“ç·´ä¸­...")

        models['gbc'] = GradientBoostingClassifier(n_estimators=200, learning_rate=0.1, max_depth=5, random_state=42)
        models['rfc'] = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)
        models['xgb'] = XGBClassifier(n_estimators=200, learning_rate=0.1, max_depth=5, random_state=42)
        models['lgbm'] = LGBMClassifier(n_estimators=200, learning_rate=0.1, max_depth=5, random_state=42, verbose=-1)
        models['catboost'] = CatBoostClassifier(iterations=200, learning_rate=0.1, depth=5, random_state=42, verbose=False)

        val_accuracies = {}
        for name, model in models.items():
            logger.info(f"  è¨“ç·´ä¸­: {name}")
            if name in ['gbc', 'rfc']:
                model.fit(X_train_scaled, y_train, sample_weight=sample_weights)
            else:
                model.fit(X_train_scaled, y_train)

            val_pred = model.predict(X_val_scaled)
            val_acc = accuracy_score(y_val, val_pred)
            val_accuracies[name] = val_acc
            logger.info(f"    æ¤œè¨¼ç²¾åº¦: {val_acc:.4f}")

        # é‡ã¿è¨ˆç®—
        weights = {name: acc / sum(val_accuracies.values()) for name, acc in val_accuracies.items()}

        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬
        ensemble_probs = np.zeros((len(X_test_scaled), 2))
        for name, model in models.items():
            probs = model.predict_proba(X_test_scaled)
            ensemble_probs += probs * weights[name]

        final_pred = ensemble_probs.argmax(axis=1)
        final_confidence = ensemble_probs.max(axis=1)

        # ä¿¡é ¼åº¦ãƒ•ã‚£ãƒ«ã‚¿
        confidence_threshold = 0.65
        confident_mask = final_confidence >= confidence_threshold
        filtered_pred = final_pred[confident_mask]
        filtered_actual = y_test[confident_mask]

        accuracy = accuracy_score(filtered_actual, filtered_pred)
        coverage = confident_mask.sum() / len(y_test) * 100

        logger.info(f"\nâœ… Phase 1.8 è¨“ç·´å®Œäº†")
        logger.info(f"  ç²¾åº¦: {accuracy*100:.2f}%")
        logger.info(f"  ã‚«ãƒãƒ¼ç‡: {coverage:.2f}%")

        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        save_data = {
            'models': models,
            'weights': weights,
            'scaler': scaler,
            'feature_columns': feature_cols,
            'confidence_threshold': confidence_threshold,
            'threshold': threshold,
            'metadata': {
                'accuracy': accuracy * 100,
                'coverage': coverage,
                'pair': self.pair
            }
        }

        save_path = self.models_dir / f'phase1_8_hourly_{self.pair.replace("=X", "")}.pkl'
        joblib.dump(save_data, save_path)
        logger.info(f"ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {save_path}")

        return save_data

    def train_phase2(self, features_df: pd.DataFrame) -> dict:
        """Phase 2: åç›Šäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«è¨“ç·´ï¼ˆæ™‚é–“è¶³ç‰ˆï¼‰"""
        logger.info("\n" + "="*80)
        logger.info("Phase 2 è¨“ç·´é–‹å§‹ï¼ˆåç›Šäºˆæ¸¬ï¼‰")
        logger.info("="*80)

        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼šæ¬¡ã®1æ™‚é–“ã®ãƒªã‚¿ãƒ¼ãƒ³
        df = self.hourly_data.load_saved_data(pair=self.pair, period='2y')
        df.index = pd.to_datetime(df.index, utc=True)
        features_df['target_return'] = ((df['close'].shift(-1) / df['close']) - 1) * 100

        # NaNé™¤å»
        feature_cols = [c for c in features_df.columns if c not in ['label_direction', 'label_direction_raw', 'actual_return', 'target_return']]
        features_df = features_df.dropna(subset=feature_cols + ['target_return'])

        X = features_df[feature_cols].values
        y = features_df['target_return'].values

        # è¨“ç·´/æ¤œè¨¼/ãƒ†ã‚¹ãƒˆåˆ†å‰²
        X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.15, shuffle=False)
        X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.176, shuffle=False)

        logger.info(f"è¨“ç·´: {len(X_train)}, æ¤œè¨¼: {len(X_val)}, ãƒ†ã‚¹ãƒˆ: {len(X_test)}")

        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        # GradientBoostingRegressorè¨“ç·´
        logger.info("\nGradientBoostingRegressor è¨“ç·´ä¸­...")
        model = GradientBoostingRegressor(
            n_estimators=300,
            learning_rate=0.05,
            max_depth=5,
            random_state=42
        )
        model.fit(X_train_scaled, y_train)

        # ãƒ†ã‚¹ãƒˆè©•ä¾¡
        y_pred = model.predict(X_test_scaled)

        # Sharpe Ratioè¨ˆç®—
        returns = y_test  # å®Ÿéš›ã®ãƒªã‚¿ãƒ¼ãƒ³
        sharpe = (returns.mean() / returns.std()) * np.sqrt(252 * 24)  # å¹´ç‡æ›ç®—ï¼ˆæ™‚é–“è¶³ï¼‰

        logger.info(f"\nâœ… Phase 2 è¨“ç·´å®Œäº†")
        logger.info(f"  Sharpe Ratio: {sharpe:.2f}")

        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        save_data = {
            'model': model,
            'scaler': scaler,
            'feature_columns': feature_cols,
            'metadata': {
                'sharpe': sharpe,
                'pair': self.pair
            }
        }

        save_path = self.models_dir / f'phase2_hourly_{self.pair.replace("=X", "")}.pkl'
        joblib.dump(save_data, save_path)
        logger.info(f"ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {save_path}")

        return save_data

    def run(self):
        """å®Œå…¨ãªè¨“ç·´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ"""
        logger.info("="*80)
        logger.info(f"æ™‚é–“è¶³ãƒ¢ãƒ‡ãƒ«è¨“ç·´é–‹å§‹: {self.pair}")
        logger.info("="*80)

        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        features_all, features_phase1 = self.load_and_prepare_data()

        # Phase 1.8 è¨“ç·´
        phase1_model = self.train_phase1_8(features_phase1)

        # Phase 2 è¨“ç·´
        phase2_model = self.train_phase2(features_all)

        logger.info("\n" + "="*80)
        logger.info("âœ… ã™ã¹ã¦ã®è¨“ç·´å®Œäº†")
        logger.info("="*80)

        return phase1_model, phase2_model


def train_all_pairs():
    """å…¨é€šè²¨ãƒšã‚¢ã®ãƒ¢ãƒ‡ãƒ«è¨“ç·´"""
    logger.info("="*80)
    logger.info("å…¨é€šè²¨ãƒšã‚¢æ™‚é–“è¶³ãƒ¢ãƒ‡ãƒ«è¨“ç·´")
    logger.info("="*80)

    pairs = ['USDJPY=X', 'EURUSD=X', 'GBPUSD=X', 'EURJPY=X']

    results = {}
    for pair in pairs:
        logger.info(f"\n\n{'#'*80}")
        logger.info(f"é€šè²¨ãƒšã‚¢: {pair}")
        logger.info(f"{'#'*80}\n")

        trainer = HourlyModelTrainer(pair=pair)
        phase1, phase2 = trainer.run()
        results[pair] = {'phase1': phase1, 'phase2': phase2}

    logger.info("\n\n" + "="*80)
    logger.info("å…¨é€šè²¨ãƒšã‚¢è¨“ç·´å®Œäº†ã‚µãƒãƒªãƒ¼")
    logger.info("="*80)

    for pair, models in results.items():
        logger.info(f"\n{pair}:")
        logger.info(f"  Phase 1.8 ç²¾åº¦: {models['phase1']['metadata']['accuracy']:.2f}%")
        logger.info(f"  Phase 1.8 ã‚«ãƒãƒ¼ç‡: {models['phase1']['metadata']['coverage']:.2f}%")
        logger.info(f"  Phase 2 Sharpe: {models['phase2']['metadata']['sharpe']:.2f}")

    return results


if __name__ == '__main__':
    train_all_pairs()
