# Phase 1 ç²¾åº¦æ”¹å–„ å®Ÿè£…ã‚¬ã‚¤ãƒ‰

## ğŸ“‹ æ¦‚è¦

ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ã€Phase 1ã®æ–¹å‘æ€§çš„ä¸­ç‡79.34%ã‹ã‚‰ã•ã‚‰ã«ç²¾åº¦ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã®**å…·ä½“çš„ãªå®Ÿè£…ã‚¬ã‚¤ãƒ‰**ã§ã™ã€‚

**ç¾åœ¨ã®åˆ°é”ç‚¹**: 79.34%ï¼ˆPhase 1.7 Ultimateï¼‰
**ç›®æ¨™**: 85-90%ï¼ˆç†è«–çš„ä¸Šé™: 90-95%ï¼‰

---

## ğŸ¯ å„ªå…ˆé †ä½ä»˜ãæ”¹å–„ç­–

| å„ªå…ˆåº¦ | æ–½ç­– | æœŸå¾…åŠ¹æœ | å®Ÿè£…é›£æ˜“åº¦ | å®Ÿè£…æœŸé–“ |
|-------|------|---------|-----------|---------|
| **1** | ãƒ©ãƒ™ãƒ«å®šç¾©ã®è¦‹ç›´ã—ï¼ˆé–¾å€¤å°å…¥ï¼‰ | +5ã€œ10% | ä½ | 1-2æ—¥ |
| **2** | ãƒ‡ãƒ¼ã‚¿é‡ã®æ‹¡å¼µï¼ˆ10å¹´åˆ†ï¼‰ | +3ã€œ5% | ä½ | 1-2æ—¥ |
| **3** | ä¿¡é ¼åº¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° | +5ã€œ8% | ä½ | 1æ—¥ |
| **4** | COTãƒ¬ãƒãƒ¼ãƒˆè¿½åŠ  | +2ã€œ4% | ä¸­ | 3-5æ—¥ |
| **5** | ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ« | +2ã€œ4% | ä¸­ | 2-3æ—¥ |
| **6** | ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æï¼ˆTwitter/ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼‰ | +3ã€œ5% | é«˜ | 1-2é€± |
| **7** | LSTMè¿½åŠ  | +1ã€œ3% | é«˜ | 1é€± |

---

## 1ï¸âƒ£ ãƒ©ãƒ™ãƒ«å®šç¾©ã®è¦‹ç›´ã—ï¼ˆå„ªå…ˆåº¦ï¼šæœ€é«˜ï¼‰

### ç¾çŠ¶ã®å•é¡Œ

```python
# ç¾åœ¨ã®å®Ÿè£…ï¼ˆphase1_6_ultimate_longterm.pyï¼‰
future_price = df['close'].shift(-lookahead_days)
df['future_direction'] = (future_price > df['close']).astype(int)
```

**å•é¡Œç‚¹**:
- 0.01å††ã®å¾®å°ãªä¸Šæ˜‡ã‚‚ã€3å††ã®å¤§å¹…ä¸Šæ˜‡ã‚‚åŒã˜ã€Œä¸Šæ˜‡(1)ã€
- ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ã®å€¤å‹•ãã‚’äºˆæ¸¬ã—ã‚ˆã†ã¨ã—ã¦ç²¾åº¦ä½ä¸‹
- **å¸‚å ´ã®ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯æˆåˆ†ã‚’å­¦ç¿’ã—ã¦ã—ã¾ã†**

---

### æ”¹å–„ç­–Aï¼šé–¾å€¤ãƒ™ãƒ¼ã‚¹åˆ†é¡ï¼ˆæ¨å¥¨ï¼‰

#### ã‚³ãƒ³ã‚»ãƒ—ãƒˆ
æœ‰æ„ãªå¤‰å‹•ï¼ˆÂ±0.5%ä»¥ä¸Šï¼‰ã®ã¿ã‚’äºˆæ¸¬å¯¾è±¡ã¨ã—ã€å¾®å°ãªå¤‰å‹•ã¯é™¤å¤–ã™ã‚‹ã€‚

#### å®Ÿè£…ã‚³ãƒ¼ãƒ‰

```python
def create_threshold_labels(df, lookahead_days=1, threshold=0.005):
    """
    é–¾å€¤ãƒ™ãƒ¼ã‚¹ã®ãƒ©ãƒ™ãƒ«ä½œæˆ

    Parameters:
    -----------
    df : pd.DataFrame
        ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿
    lookahead_days : int
        ä½•æ—¥å…ˆã‚’äºˆæ¸¬ã™ã‚‹ã‹
    threshold : float
        é–¾å€¤ï¼ˆ0.005 = 0.5%ï¼‰

    Returns:
    --------
    labels : pd.Series
        1=æœ‰æ„ãªä¸Šæ˜‡, 0=æœ‰æ„ãªä¸‹é™, -1=é™¤å¤–ï¼ˆä¸­ç«‹ï¼‰
    """
    future_price = df['close'].shift(-lookahead_days)
    future_return = (future_price - df['close']) / df['close']

    # ãƒ©ãƒ™ãƒ«ä½œæˆ
    labels = pd.Series(index=df.index, dtype=int)
    labels[future_return > threshold] = 1      # æœ‰æ„ãªä¸Šæ˜‡
    labels[future_return < -threshold] = 0     # æœ‰æ„ãªä¸‹é™
    labels[np.abs(future_return) <= threshold] = -1  # ä¸­ç«‹ï¼ˆé™¤å¤–ï¼‰

    return labels, future_return

# ä½¿ç”¨ä¾‹
df['label'], df['return'] = create_threshold_labels(df, lookahead_days=1, threshold=0.005)

# ä¸­ç«‹ã‚’é™¤å¤–ã—ã¦å­¦ç¿’
mask = df['label'] != -1
X_train = X[mask]
y_train = df['label'][mask]

print(f"å…ƒã®ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(df)}")
print(f"å­¦ç¿’ã‚µãƒ³ãƒ—ãƒ«æ•°: {mask.sum()} ({mask.sum()/len(df)*100:.1f}%)")
print(f"é™¤å¤–ã‚µãƒ³ãƒ—ãƒ«æ•°: {(~mask).sum()} ({(~mask).sum()/len(df)*100:.1f}%)")
```

#### é–¾å€¤ã®æœ€é©åŒ–

```python
def optimize_threshold(df, X, thresholds=[0.003, 0.005, 0.007, 0.01, 0.015]):
    """
    è¤‡æ•°ã®é–¾å€¤ã§ç²¾åº¦ã‚’æ¯”è¼ƒ
    """
    results = []

    for thresh in thresholds:
        labels, _ = create_threshold_labels(df, threshold=thresh)
        mask = labels != -1

        if mask.sum() < 100:  # ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒå°‘ãªã™ãã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            continue

        X_filtered = X[mask]
        y_filtered = labels[mask]

        # Train/Teståˆ†å‰²
        split_idx = int(len(X_filtered) * 0.85)
        X_train, X_test = X_filtered[:split_idx], X_filtered[split_idx:]
        y_train, y_test = y_filtered[:split_idx], y_filtered[split_idx:]

        # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
        model = XGBClassifier(n_estimators=100, max_depth=5)
        model.fit(X_train, y_train)

        # è©•ä¾¡
        accuracy = model.score(X_test, y_test)
        coverage = mask.sum() / len(mask)

        results.append({
            'threshold': thresh,
            'accuracy': accuracy,
            'coverage': coverage,
            'samples': mask.sum()
        })

    return pd.DataFrame(results)

# å®Ÿè¡Œ
results = optimize_threshold(df, X)
print(results)
```

#### æœŸå¾…ã•ã‚Œã‚‹çµæœ

```
é–¾å€¤0.3%:  ç²¾åº¦82%, ã‚«ãƒãƒ¼ç‡85%ï¼ˆã‚µãƒ³ãƒ—ãƒ«æ•°å¤šã€ç²¾åº¦ä¸­ï¼‰
é–¾å€¤0.5%:  ç²¾åº¦85%, ã‚«ãƒãƒ¼ç‡70%ï¼ˆæ¨å¥¨ãƒãƒ©ãƒ³ã‚¹ï¼‰â­
é–¾å€¤0.7%:  ç²¾åº¦87%, ã‚«ãƒãƒ¼ç‡55%ï¼ˆç²¾åº¦é«˜ã€ã‚µãƒ³ãƒ—ãƒ«å°‘ï¼‰
é–¾å€¤1.0%:  ç²¾åº¦90%, ã‚«ãƒãƒ¼ç‡40%ï¼ˆç²¾åº¦æœ€é«˜ã€å®Ÿç”¨æ€§ä½ï¼‰
```

**æ¨å¥¨**: é–¾å€¤0.5%ï¼ˆã‚«ãƒãƒ¼ç‡70%ã€ç²¾åº¦85%ï¼‰

---

### æ”¹å–„ç­–Bï¼š3ã‚¯ãƒ©ã‚¹åˆ†é¡

#### ã‚³ãƒ³ã‚»ãƒ—ãƒˆ
å¤§å¹…ä¸Šæ˜‡ãƒ»æ¨ªã°ã„ãƒ»å¤§å¹…ä¸‹é™ã®3ã‚¯ãƒ©ã‚¹ã«åˆ†é¡ã€‚

#### å®Ÿè£…ã‚³ãƒ¼ãƒ‰

```python
def create_3class_labels(df, lookahead_days=1, threshold=0.01):
    """
    3ã‚¯ãƒ©ã‚¹åˆ†é¡ãƒ©ãƒ™ãƒ«ä½œæˆ

    Returns:
    --------
    labels : 2=å¤§å¹…ä¸Šæ˜‡, 1=æ¨ªã°ã„, 0=å¤§å¹…ä¸‹é™
    """
    future_price = df['close'].shift(-lookahead_days)
    future_return = (future_price - df['close']) / df['close']

    labels = pd.Series(index=df.index, dtype=int)
    labels[future_return > threshold] = 2       # å¤§å¹…ä¸Šæ˜‡
    labels[future_return < -threshold] = 0      # å¤§å¹…ä¸‹é™
    labels[(future_return >= -threshold) & (future_return <= threshold)] = 1  # æ¨ªã°ã„

    return labels

# ä½¿ç”¨ä¾‹
df['label_3class'] = create_3class_labels(df, threshold=0.01)

# åˆ†å¸ƒç¢ºèª
print(df['label_3class'].value_counts())

# ãƒ¢ãƒ‡ãƒ«è¨“ç·´
model = XGBClassifier(n_estimators=100, max_depth=5, objective='multi:softmax', num_class=3)
model.fit(X_train, y_train)

# äºˆæ¸¬
predictions = model.predict(X_test)
probabilities = model.predict_proba(X_test)

# ã€Œæ¨ªã°ã„ã€ã‚’é™¤å¤–ã—ãŸç²¾åº¦ã‚‚è¨ˆç®—
mask = (y_test != 1)  # æ¨ªã°ã„ã‚’é™¤å¤–
accuracy_without_neutral = accuracy_score(y_test[mask], predictions[mask])
print(f"æ¨ªã°ã„é™¤å¤–ç²¾åº¦: {accuracy_without_neutral:.2%}")
```

---

## 2ï¸âƒ£ ãƒ‡ãƒ¼ã‚¿é‡ã®æ‹¡å¼µï¼ˆå„ªå…ˆåº¦ï¼šé«˜ï¼‰

### ç¾çŠ¶
- 799æ—¥ï¼ˆç´„3å¹´ï¼‰ã®ãƒ‡ãƒ¼ã‚¿
- è¨“ç·´ã‚µãƒ³ãƒ—ãƒ«: 559ä»¶

### æ”¹å–„ç›®æ¨™
- **2,500æ—¥ï¼ˆç´„10å¹´ï¼‰ã®ãƒ‡ãƒ¼ã‚¿**
- è¨“ç·´ã‚µãƒ³ãƒ—ãƒ«: 1,750ä»¶ï¼ˆ3.1å€ï¼‰

---

### å®Ÿè£…ã‚³ãƒ¼ãƒ‰

```python
from src.data_sources.yahoo_finance import YahooFinanceData
from datetime import datetime, timedelta

def get_extended_data(years_back=10, instrument="USD/JPY"):
    """
    10å¹´åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    """
    yf_data = YahooFinanceData()

    end_date = datetime.now()
    start_date = end_date - timedelta(days=years_back * 365)

    # ãƒ¡ã‚¤ãƒ³é€šè²¨ãƒšã‚¢
    main_data = yf_data.get_forex_data(
        pair=instrument.replace('_', '/'),
        start_date=start_date.strftime('%Y-%m-%d'),
        end_date=end_date.strftime('%Y-%m-%d'),
        interval='1d'
    )

    print(f"å–å¾—ãƒ‡ãƒ¼ã‚¿æœŸé–“: {start_date.date()} - {end_date.date()}")
    print(f"ãƒ‡ãƒ¼ã‚¿æ—¥æ•°: {len(main_data)}æ—¥")

    return main_data

# å®Ÿè¡Œä¾‹
df_10years = get_extended_data(years_back=10, instrument="USD/JPY")
```

---

### ãƒ‡ãƒ¼ã‚¿æœŸé–“åˆ¥ã®é‡ã¿ä»˜ã‘ï¼ˆæ¨å¥¨ï¼‰

å¤ã„ãƒ‡ãƒ¼ã‚¿ã¯å¸‚å ´æ§‹é€ ãŒç•°ãªã‚‹ãŸã‚ã€**ç›´è¿‘ãƒ‡ãƒ¼ã‚¿ã‚’é‡è¦–**ã™ã‚‹é‡ã¿ä»˜ã‘ã‚’è¡Œã†ã€‚

```python
def create_time_weighted_sample(df, decay_rate=0.95):
    """
    æ™‚ç³»åˆ—ã«æ²¿ã£ã¦é‡ã¿ã‚’ä»˜ã‘ã‚‹

    ç›´è¿‘ãƒ‡ãƒ¼ã‚¿ã»ã©é‡ã¿ãŒå¤§ãã„ï¼ˆæŒ‡æ•°æ¸›è¡°ï¼‰

    Parameters:
    -----------
    df : pd.DataFrame
        æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ï¼ˆå¤ã„é †ã«ã‚½ãƒ¼ãƒˆæ¸ˆã¿ï¼‰
    decay_rate : float
        æ¸›è¡°ç‡ï¼ˆ0.95æ¨å¥¨ï¼‰

    Returns:
    --------
    weights : np.array
        å„ã‚µãƒ³ãƒ—ãƒ«ã®é‡ã¿
    """
    n = len(df)
    # ç›´è¿‘ãŒ1ã€æœ€å¤ãŒdecay_rate^nã¨ãªã‚‹é‡ã¿
    weights = np.array([decay_rate ** (n - i - 1) for i in range(n)])

    # æ­£è¦åŒ–ï¼ˆåˆè¨ˆã‚’1ã«ï¼‰
    weights = weights / weights.sum()

    return weights

# ä½¿ç”¨ä¾‹
sample_weights = create_time_weighted_sample(df, decay_rate=0.95)

# XGBoostã§é‡ã¿ä»˜ãå­¦ç¿’
model = XGBClassifier(n_estimators=100, max_depth=5)
model.fit(X_train, y_train, sample_weight=sample_weights[:len(X_train)])
```

---

### å¸‚å ´ãƒ¬ã‚¸ãƒ¼ãƒ åˆ†å‰²ï¼ˆé«˜åº¦ãªæ‰‹æ³•ï¼‰

å¸‚å ´ã‚’ã€Œä¸Šæ˜‡ãƒˆãƒ¬ãƒ³ãƒ‰æœŸã€ã€Œä¸‹é™ãƒˆãƒ¬ãƒ³ãƒ‰æœŸã€ã€Œãƒ¬ãƒ³ã‚¸ç›¸å ´æœŸã€ã«åˆ†å‰²ã—ã€å„æœŸé–“ã‹ã‚‰å‡ç­‰ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€‚

```python
def identify_market_regime(df, window=60):
    """
    å¸‚å ´ãƒ¬ã‚¸ãƒ¼ãƒ ã®è­˜åˆ¥

    Returns:
    --------
    regime : 'uptrend', 'downtrend', 'range'
    """
    # 60æ—¥ç§»å‹•å¹³å‡ã®å‚¾ã
    ma60 = df['close'].rolling(window=window).mean()
    slope = ma60.diff(window)

    # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
    volatility = df['close'].pct_change().rolling(window=window).std()

    # ãƒ¬ã‚¸ãƒ¼ãƒ åˆ¤å®š
    regime = pd.Series(index=df.index, dtype=str)
    regime[slope > 0.02] = 'uptrend'      # ä¸Šæ˜‡ãƒˆãƒ¬ãƒ³ãƒ‰
    regime[slope < -0.02] = 'downtrend'   # ä¸‹é™ãƒˆãƒ¬ãƒ³ãƒ‰
    regime[(slope >= -0.02) & (slope <= 0.02)] = 'range'  # ãƒ¬ãƒ³ã‚¸

    return regime

# ä½¿ç”¨ä¾‹
df['regime'] = identify_market_regime(df, window=60)

# ãƒ¬ã‚¸ãƒ¼ãƒ åˆ¥ã®ã‚µãƒ³ãƒ—ãƒ«æ•°ç¢ºèª
print(df['regime'].value_counts())

# å„ãƒ¬ã‚¸ãƒ¼ãƒ ã‹ã‚‰å‡ç­‰ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
from sklearn.utils import resample

regime_samples = []
for regime in ['uptrend', 'downtrend', 'range']:
    regime_data = df[df['regime'] == regime]
    # å„ãƒ¬ã‚¸ãƒ¼ãƒ ã‹ã‚‰200ã‚µãƒ³ãƒ—ãƒ«ãšã¤
    sampled = resample(regime_data, n_samples=min(200, len(regime_data)), random_state=42)
    regime_samples.append(sampled)

balanced_df = pd.concat(regime_samples)
```

---

## 3ï¸âƒ£ ä¿¡é ¼åº¦ãƒ™ãƒ¼ã‚¹ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆå„ªå…ˆåº¦ï¼šé«˜ï¼‰

### ã‚³ãƒ³ã‚»ãƒ—ãƒˆ
ç¢ºä¿¡åº¦ã®é«˜ã„äºˆæ¸¬ã®ã¿ã‚’æ¡ç”¨ã—ã€**ç²¾åº¦ã‚’å„ªå…ˆ**ã™ã‚‹ã€‚

---

### å®Ÿè£…ã‚³ãƒ¼ãƒ‰

```python
def predict_with_confidence(model, X, threshold=0.65):
    """
    ä¿¡é ¼åº¦ãƒ™ãƒ¼ã‚¹ã®äºˆæ¸¬

    Parameters:
    -----------
    model : sklearn model
        è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
    X : array-like
        ç‰¹å¾´é‡
    threshold : float
        ä¿¡é ¼åº¦é–¾å€¤ï¼ˆ0.65æ¨å¥¨ï¼‰

    Returns:
    --------
    predictions : np.array
        1=ä¸Šæ˜‡äºˆæ¸¬, 0=ä¸‹é™äºˆæ¸¬, -1=è¦‹é€ã‚Š
    probabilities : np.array
        ç¢ºç‡å€¤
    """
    probabilities = model.predict_proba(X)[:, 1]  # ä¸Šæ˜‡ç¢ºç‡

    predictions = np.full(len(probabilities), -1)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: è¦‹é€ã‚Š
    predictions[probabilities > threshold] = 1      # ä¸Šæ˜‡äºˆæ¸¬ï¼ˆç¢ºä¿¡ã‚ã‚Šï¼‰
    predictions[probabilities < (1 - threshold)] = 0  # ä¸‹é™äºˆæ¸¬ï¼ˆç¢ºä¿¡ã‚ã‚Šï¼‰

    return predictions, probabilities

# ä½¿ç”¨ä¾‹
predictions, probabilities = predict_with_confidence(model, X_test, threshold=0.65)

# è¦‹é€ã‚Šã‚’é™¤å¤–ã—ãŸç²¾åº¦è¨ˆç®—
mask = predictions != -1
accuracy = accuracy_score(y_test[mask], predictions[mask])
coverage = mask.sum() / len(mask)

print(f"ä¿¡é ¼åº¦é–¾å€¤: 0.65")
print(f"ç²¾åº¦: {accuracy:.2%}")
print(f"ã‚«ãƒãƒ¼ç‡: {coverage:.2%}")
print(f"è¦‹é€ã‚Š: {(~mask).sum()}ä»¶ ({(~mask).sum()/len(mask)*100:.1f}%)")
```

---

### é–¾å€¤æœ€é©åŒ–

```python
def optimize_confidence_threshold(model, X_test, y_test):
    """
    ä¿¡é ¼åº¦é–¾å€¤ã‚’æœ€é©åŒ–
    """
    thresholds = np.arange(0.5, 0.85, 0.05)
    results = []

    for thresh in thresholds:
        predictions, probabilities = predict_with_confidence(model, X_test, threshold=thresh)

        mask = predictions != -1
        if mask.sum() == 0:
            continue

        accuracy = accuracy_score(y_test[mask], predictions[mask])
        coverage = mask.sum() / len(mask)

        # æœŸå¾…å€¤è¨ˆç®—ï¼ˆç²¾åº¦ Ã— ã‚«ãƒãƒ¼ç‡ï¼‰
        expected_value = accuracy * coverage

        results.append({
            'threshold': thresh,
            'accuracy': accuracy,
            'coverage': coverage,
            'expected_value': expected_value,
            'predictions': mask.sum()
        })

    df_results = pd.DataFrame(results)

    # æœŸå¾…å€¤ãŒæœ€å¤§ã®é–¾å€¤ã‚’æ¨å¥¨
    best_idx = df_results['expected_value'].idxmax()
    best_threshold = df_results.loc[best_idx, 'threshold']

    print("ä¿¡é ¼åº¦é–¾å€¤åˆ¥ã®çµæœ:")
    print(df_results)
    print(f"\næ¨å¥¨é–¾å€¤: {best_threshold}")

    return df_results

# å®Ÿè¡Œ
results = optimize_confidence_threshold(model, X_test, y_test)
```

---

### ã‚°ãƒ©ãƒ•å¯è¦–åŒ–

```python
import matplotlib.pyplot as plt

def plot_confidence_analysis(results_df):
    """
    ä¿¡é ¼åº¦åˆ†æã®å¯è¦–åŒ–
    """
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    # å·¦: ç²¾åº¦ã¨ã‚«ãƒãƒ¼ç‡
    ax1 = axes[0]
    ax1.plot(results_df['threshold'], results_df['accuracy'], 'o-', label='ç²¾åº¦', linewidth=2)
    ax1.plot(results_df['threshold'], results_df['coverage'], 's-', label='ã‚«ãƒãƒ¼ç‡', linewidth=2)
    ax1.set_xlabel('ä¿¡é ¼åº¦é–¾å€¤', fontsize=12)
    ax1.set_ylabel('å€¤', fontsize=12)
    ax1.set_title('ä¿¡é ¼åº¦é–¾å€¤ vs ç²¾åº¦ãƒ»ã‚«ãƒãƒ¼ç‡', fontsize=14)
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # å³: æœŸå¾…å€¤
    ax2 = axes[1]
    ax2.plot(results_df['threshold'], results_df['expected_value'], 'D-', color='green', linewidth=2)
    best_idx = results_df['expected_value'].idxmax()
    best_thresh = results_df.loc[best_idx, 'threshold']
    best_ev = results_df.loc[best_idx, 'expected_value']
    ax2.scatter([best_thresh], [best_ev], color='red', s=200, zorder=5, label=f'æœ€é©: {best_thresh}')
    ax2.set_xlabel('ä¿¡é ¼åº¦é–¾å€¤', fontsize=12)
    ax2.set_ylabel('æœŸå¾…å€¤ï¼ˆç²¾åº¦ Ã— ã‚«ãƒãƒ¼ç‡ï¼‰', fontsize=12)
    ax2.set_title('ä¿¡é ¼åº¦é–¾å€¤ vs æœŸå¾…å€¤', fontsize=14)
    ax2.legend()
    ax2.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('outputs/confidence_analysis.png', dpi=150)
    plt.close()

# å®Ÿè¡Œ
plot_confidence_analysis(results)
```

---

## 4ï¸âƒ£ COTãƒ¬ãƒãƒ¼ãƒˆã®è¿½åŠ ï¼ˆå„ªå…ˆåº¦ï¼šä¸­ï¼‰

### COTï¼ˆCommitment of Tradersï¼‰ãƒ¬ãƒãƒ¼ãƒˆã¨ã¯

CFTCï¼ˆç±³å•†å“å…ˆç‰©å–å¼•å§”å“¡ä¼šï¼‰ãŒæ¯é€±ç™ºè¡¨ã™ã‚‹ã€å¤§å£æŠ•æ©Ÿç­‹ã®ãƒã‚¸ã‚·ãƒ§ãƒ³æƒ…å ±ã€‚

**é‡è¦æ€§**:
- å¤§å£æŠ•æ©Ÿç­‹ï¼ˆãƒ˜ãƒƒã‚¸ãƒ•ã‚¡ãƒ³ãƒ‰ç­‰ï¼‰ã®å‹•å‘ãŒã‚ã‹ã‚‹
- **å…ˆè¡ŒæŒ‡æ¨™**ã¨ã—ã¦æ©Ÿèƒ½ï¼ˆå¤§å£ãŒå…ˆã«å‹•ãï¼‰
- æ¥µç«¯ãªãƒã‚¸ã‚·ãƒ§ãƒ³åã‚Šã¯åè»¢ã®ã‚µã‚¤ãƒ³

---

### ãƒ‡ãƒ¼ã‚¿å–å¾—

```python
import pandas as pd
import requests
from io import StringIO

def fetch_cot_data(start_year=2014):
    """
    COTãƒ¬ãƒãƒ¼ãƒˆã‚’å–å¾—

    CFTCã®å…¬å¼ã‚µã‚¤ãƒˆã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    """
    # CFTC COT Historical Data
    # Japanese Yen Futures (Code: 097741)
    url = "https://www.cftc.gov/files/dea/history/fut_fin_txt_{}.zip"

    all_data = []
    current_year = datetime.now().year

    for year in range(start_year, current_year + 1):
        try:
            # ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            response = requests.get(url.format(year))
            # ZIPè§£å‡ã¨èª­ã¿è¾¼ã¿
            # (å®Ÿè£…çœç•¥ - å®Ÿéš›ã¯zipãƒ•ã‚¡ã‚¤ãƒ«ã®è§£å‡ãŒå¿…è¦)

            # Japanese Yenï¼ˆã‚³ãƒ¼ãƒ‰097741ï¼‰ã®ã¿æŠ½å‡º
            # ...

        except Exception as e:
            print(f"Error fetching {year}: {e}")
            continue

    df_cot = pd.concat(all_data)
    return df_cot

# ç°¡æ˜“ç‰ˆ: æ‰‹å‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸCSVã‚’èª­ã¿è¾¼ã¿
df_cot = pd.read_csv('cot_jpy_data.csv', parse_dates=['date'])
```

---

### ç‰¹å¾´é‡ä½œæˆ

```python
def create_cot_features(df_cot):
    """
    COTãƒ¬ãƒãƒ¼ãƒˆã‹ã‚‰ç‰¹å¾´é‡ã‚’ä½œæˆ
    """
    df_cot = df_cot.sort_values('date').reset_index(drop=True)

    # 1. ãƒãƒƒãƒˆãƒã‚¸ã‚·ãƒ§ãƒ³ï¼ˆãƒ­ãƒ³ã‚° - ã‚·ãƒ§ãƒ¼ãƒˆï¼‰
    df_cot['speculator_net'] = df_cot['speculator_long'] - df_cot['speculator_short']
    df_cot['commercial_net'] = df_cot['commercial_long'] - df_cot['commercial_short']

    # 2. ãƒãƒƒãƒˆãƒã‚¸ã‚·ãƒ§ãƒ³ã®å¤‰åŒ–ç‡
    df_cot['speculator_net_change'] = df_cot['speculator_net'].pct_change()
    df_cot['commercial_net_change'] = df_cot['commercial_net'].pct_change()

    # 3. ãƒã‚¸ã‚·ãƒ§ãƒ³ã®åã‚Šåº¦ï¼ˆæ¥µç«¯ãªåã‚Šã¯åè»¢ã®ã‚µã‚¤ãƒ³ï¼‰
    df_cot['speculator_net_zscore'] = (
        df_cot['speculator_net'] - df_cot['speculator_net'].rolling(52).mean()
    ) / df_cot['speculator_net'].rolling(52).std()

    # 4. ãƒ­ãƒ³ã‚°ã¨ã‚·ãƒ§ãƒ¼ãƒˆã®æ¯”ç‡
    df_cot['speculator_long_short_ratio'] = (
        df_cot['speculator_long'] / (df_cot['speculator_short'] + 1e-10)
    )

    return df_cot

# ä½¿ç”¨ä¾‹
df_cot = create_cot_features(df_cot)
```

---

### ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã¨ã®çµ±åˆ

```python
def merge_cot_with_price_data(df_price, df_cot):
    """
    ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã¨COTãƒ¬ãƒãƒ¼ãƒˆã‚’çµ±åˆ

    COTã¯é€±æ¬¡ãƒ‡ãƒ¼ã‚¿ãªã®ã§ã€æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿ã«å‰æ–¹åŸ‹ã‚
    """
    # æ—¥ä»˜ã§ãƒãƒ¼ã‚¸
    df_merged = pd.merge_asof(
        df_price.sort_values('date'),
        df_cot[['date', 'speculator_net', 'speculator_net_change',
                'speculator_net_zscore', 'speculator_long_short_ratio']],
        on='date',
        direction='backward'  # ç›´è¿‘ã®COTãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
    )

    return df_merged

# å®Ÿè¡Œ
df = merge_cot_with_price_data(df_price, df_cot)

print("è¿½åŠ ã•ã‚ŒãŸCOTç‰¹å¾´é‡:")
print(['speculator_net', 'speculator_net_change', 'speculator_net_zscore', 'speculator_long_short_ratio'])
```

---

### COTç‰¹å¾´é‡ã®æœ‰åŠ¹æ€§æ¤œè¨¼

```python
from sklearn.ensemble import RandomForestClassifier

def evaluate_cot_features(df, X_original, y):
    """
    COTç‰¹å¾´é‡ã®é‡è¦åº¦ã‚’è©•ä¾¡
    """
    # COTç‰¹å¾´é‡ã®ã¿ã§ãƒ¢ãƒ‡ãƒ«è¨“ç·´
    cot_features = ['speculator_net', 'speculator_net_change',
                    'speculator_net_zscore', 'speculator_long_short_ratio']

    X_cot = df[cot_features].fillna(0)

    # Train/Teståˆ†å‰²
    split_idx = int(len(X_cot) * 0.85)
    X_train, X_test = X_cot[:split_idx], X_cot[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]

    # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
    model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)
    model.fit(X_train, y_train)

    accuracy_cot_only = model.score(X_test, y_test)

    # å…¨ç‰¹å¾´é‡ã¨æ¯”è¼ƒ
    model_full = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)
    X_full = pd.concat([X_original, X_cot], axis=1)
    model_full.fit(X_full[:split_idx], y_train)
    accuracy_full = model_full.score(X_full[split_idx:], y_test)

    print("COTç‰¹å¾´é‡ã®è©•ä¾¡:")
    print(f"  COTç‰¹å¾´é‡ã®ã¿: {accuracy_cot_only:.2%}")
    print(f"  å…¨ç‰¹å¾´é‡: {accuracy_full:.2%}")
    print(f"  æ”¹å–„: {(accuracy_full - accuracy_cot_only):.2%}")

    return model_full

# å®Ÿè¡Œ
model_with_cot = evaluate_cot_features(df, X_original, y)
```

---

## 5ï¸âƒ£ ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ï¼ˆå„ªå…ˆåº¦ï¼šä¸­ï¼‰

### ã‚³ãƒ³ã‚»ãƒ—ãƒˆ
2æ®µéšã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ï¼š
1. **Level 1**: å¤šæ§˜ãªãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ç¾¤
2. **Level 2**: Level 1ã®å‡ºåŠ›ã‚’å…¥åŠ›ã¨ã™ã‚‹ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«

**ãƒ¡ãƒªãƒƒãƒˆ**: ãƒ¢ãƒ‡ãƒ«ã®å¤šæ§˜æ€§ãŒå¢—ã—ã€éå­¦ç¿’ã‚’é˜²ãã¤ã¤ç²¾åº¦å‘ä¸Š

---

### å®Ÿè£…ã‚³ãƒ¼ãƒ‰

```python
from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier

def build_stacking_model():
    """
    ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
    """
    # Level 1: ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼ˆå¤šæ§˜æ€§ã‚’é‡è¦–ï¼‰
    base_models = [
        ('gbc', GradientBoostingClassifier(n_estimators=100, max_depth=5, random_state=42)),
        ('rf', RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)),
        ('xgb', XGBClassifier(n_estimators=100, max_depth=5, random_state=42, eval_metric='logloss')),
        ('lgb', LGBMClassifier(n_estimators=100, max_depth=5, random_state=42, verbose=-1)),
        ('cat', CatBoostClassifier(iterations=100, depth=5, random_state=42, verbose=False)),
    ]

    # Level 2: ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ãªãƒ¢ãƒ‡ãƒ«ã‚’æ¨å¥¨ï¼‰
    meta_model = LogisticRegression(max_iter=1000, random_state=42)

    # ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°
    stacking_clf = StackingClassifier(
        estimators=base_models,
        final_estimator=meta_model,
        cv=5,  # 5-fold Cross Validation
        stack_method='predict_proba',  # ç¢ºç‡ã‚’ä½¿ç”¨
        passthrough=True  # å…ƒã®ç‰¹å¾´é‡ã‚‚ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã«æ¸¡ã™
    )

    return stacking_clf

# ä½¿ç”¨ä¾‹
stacking_model = build_stacking_model()
stacking_model.fit(X_train, y_train)

# è©•ä¾¡
accuracy = stacking_model.score(X_test, y_test)
print(f"ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ç²¾åº¦: {accuracy:.2%}")
```

---

### ç‰¹å¾´é‡ã®ã‚µãƒ–ã‚»ãƒƒãƒˆåŒ–ï¼ˆé«˜åº¦ï¼‰

å„ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã«ç•°ãªã‚‹ç‰¹å¾´é‡ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ã€ã•ã‚‰ã«å¤šæ§˜æ€§ã‚’å¢—ã™ã€‚

```python
from sklearn.base import BaseEstimator, ClassifierMixin

class FeatureSubsetClassifier(BaseEstimator, ClassifierMixin):
    """
    ç‰¹å®šã®ç‰¹å¾´é‡ã‚µãƒ–ã‚»ãƒƒãƒˆã®ã¿ã‚’ä½¿ç”¨ã™ã‚‹ãƒ©ãƒƒãƒ‘ãƒ¼
    """
    def __init__(self, estimator, feature_indices):
        self.estimator = estimator
        self.feature_indices = feature_indices

    def fit(self, X, y):
        X_subset = X[:, self.feature_indices]
        self.estimator.fit(X_subset, y)
        return self

    def predict(self, X):
        X_subset = X[:, self.feature_indices]
        return self.estimator.predict(X_subset)

    def predict_proba(self, X):
        X_subset = X[:, self.feature_indices]
        return self.estimator.predict_proba(X_subset)

# ç‰¹å¾´é‡ã‚’3ã‚°ãƒ«ãƒ¼ãƒ—ã«åˆ†å‰²
n_features = X_train.shape[1]
group1 = list(range(0, n_features // 3))                    # ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™
group2 = list(range(n_features // 3, 2 * n_features // 3))  # çµ±è¨ˆç‰¹å¾´
group3 = list(range(2 * n_features // 3, n_features))       # ã‚¯ãƒ­ã‚¹é€šè²¨ãƒ»çµŒæ¸ˆæŒ‡æ¨™

# å„ãƒ¢ãƒ‡ãƒ«ã«ç•°ãªã‚‹ç‰¹å¾´é‡ã‚»ãƒƒãƒˆã‚’å‰²ã‚Šå½“ã¦
base_models = [
    ('xgb_tech', FeatureSubsetClassifier(XGBClassifier(), group1)),
    ('lgb_stat', FeatureSubsetClassifier(LGBMClassifier(), group2)),
    ('cat_econ', FeatureSubsetClassifier(CatBoostClassifier(verbose=False), group3)),
    ('rf_all', RandomForestClassifier()),  # å…¨ç‰¹å¾´é‡ä½¿ç”¨
]

stacking_clf = StackingClassifier(
    estimators=base_models,
    final_estimator=LogisticRegression(),
    cv=5
)
```

---

## 6ï¸âƒ£ LSTMè¿½åŠ ï¼ˆå„ªå…ˆåº¦ï¼šä½ã€œä¸­ï¼‰

### æ³¨æ„äº‹é …
LSTMã¯æ™‚ç³»åˆ—ã®é•·æœŸä¾å­˜é–¢ä¿‚ã‚’æ•æ‰ã§ãã‚‹ãŒã€**å¤§é‡ãƒ‡ãƒ¼ã‚¿ï¼ˆ1,000+ã‚µãƒ³ãƒ—ãƒ«ï¼‰ãŒå¿…è¦**ã€‚

ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿é‡ï¼ˆ799æ—¥ï¼‰ã§ã¯åŠ¹æœè–„ã€‚**10å¹´åˆ†ãƒ‡ãƒ¼ã‚¿å–å¾—å¾Œ**ã«å®Ÿè£…æ¨å¥¨ã€‚

---

### å®Ÿè£…ã‚³ãƒ¼ãƒ‰

```python
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

def create_sequences(X, y, sequence_length=20):
    """
    æ™‚ç³»åˆ—ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ä½œæˆ

    Parameters:
    -----------
    X : np.array
        ç‰¹å¾´é‡ (samples, features)
    y : np.array
        ãƒ©ãƒ™ãƒ«
    sequence_length : int
        ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ï¼ˆéå»ä½•æ—¥åˆ†ã‚’è¦‹ã‚‹ã‹ï¼‰

    Returns:
    --------
    X_seq : np.array (samples - sequence_length, sequence_length, features)
    y_seq : np.array (samples - sequence_length,)
    """
    X_seq, y_seq = [], []

    for i in range(sequence_length, len(X)):
        X_seq.append(X[i - sequence_length:i])
        y_seq.append(y[i])

    return np.array(X_seq), np.array(y_seq)

def build_lstm_model(sequence_length, n_features):
    """
    LSTMãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
    """
    model = Sequential([
        LSTM(64, return_sequences=True, input_shape=(sequence_length, n_features)),
        Dropout(0.3),
        BatchNormalization(),

        LSTM(32, return_sequences=False),
        Dropout(0.3),
        BatchNormalization(),

        Dense(16, activation='relu'),
        Dropout(0.2),

        Dense(1, activation='sigmoid')
    ])

    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
        loss='binary_crossentropy',
        metrics=['accuracy']
    )

    return model

# ä½¿ç”¨ä¾‹
sequence_length = 20
n_features = X_train.shape[1]

# ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä½œæˆ
X_train_seq, y_train_seq = create_sequences(X_train, y_train, sequence_length)
X_test_seq, y_test_seq = create_sequences(X_test, y_test, sequence_length)

print(f"X_train_seq shape: {X_train_seq.shape}")  # (samples, 20, n_features)
print(f"y_train_seq shape: {y_train_seq.shape}")  # (samples,)

# ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
lstm_model = build_lstm_model(sequence_length, n_features)
lstm_model.summary()

# ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
callbacks = [
    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),
    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)
]

# è¨“ç·´
history = lstm_model.fit(
    X_train_seq, y_train_seq,
    validation_split=0.15,
    epochs=100,
    batch_size=32,
    callbacks=callbacks,
    verbose=1
)

# è©•ä¾¡
loss, accuracy = lstm_model.evaluate(X_test_seq, y_test_seq)
print(f"LSTMç²¾åº¦: {accuracy:.2%}")
```

---

### LSTMã¨æ±ºå®šæœ¨ã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰

```python
def build_hybrid_model(X_train, y_train, X_test, y_test, sequence_length=20):
    """
    LSTM + æ±ºå®šæœ¨ã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
    """
    # 1. LSTMãƒ¢ãƒ‡ãƒ«
    X_train_seq, y_train_seq = create_sequences(X_train, y_train, sequence_length)
    X_test_seq, y_test_seq = create_sequences(X_test, y_test, sequence_length)

    lstm_model = build_lstm_model(sequence_length, X_train.shape[1])
    lstm_model.fit(X_train_seq, y_train_seq, validation_split=0.15, epochs=50, verbose=0)

    # LSTMäºˆæ¸¬ï¼ˆç¢ºç‡ï¼‰
    lstm_proba = lstm_model.predict(X_test_seq)

    # 2. XGBoostãƒ¢ãƒ‡ãƒ«
    xgb_model = XGBClassifier(n_estimators=100, max_depth=5)
    # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·åˆ†ã‚’ã‚¹ã‚­ãƒƒãƒ—
    xgb_model.fit(X_train[sequence_length:], y_train[sequence_length:])
    xgb_proba = xgb_model.predict_proba(X_test[sequence_length:])[:, 1]

    # 3. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ï¼ˆé‡ã¿ä»˜ãå¹³å‡ï¼‰
    ensemble_proba = 0.5 * lstm_proba.flatten() + 0.5 * xgb_proba
    ensemble_pred = (ensemble_proba > 0.5).astype(int)

    accuracy = accuracy_score(y_test[sequence_length:], ensemble_pred)
    print(f"ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ç²¾åº¦: {accuracy:.2%}")

    return lstm_model, xgb_model

# å®Ÿè¡Œ
lstm_model, xgb_model = build_hybrid_model(X_train, y_train, X_test, y_test)
```

---

## 7ï¸âƒ£ ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æï¼ˆå„ªå…ˆåº¦ï¼šä¸­ã€œé«˜ï¼‰

### Twitteræ„Ÿæƒ…åˆ†æ

```python
# æ³¨: Twitter API v2ãŒå¿…è¦ï¼ˆç„¡æ–™æ ã‚ã‚Šï¼‰
import tweepy
from textblob import TextBlob

def fetch_forex_sentiment(api_key, api_secret, keywords=['USDJPY', 'forex', 'å††é«˜', 'å††å®‰']):
    """
    Twitterã‹ã‚‰ç‚ºæ›¿é–¢é€£ãƒ„ã‚¤ãƒ¼ãƒˆã‚’å–å¾—ã—ã€æ„Ÿæƒ…åˆ†æ
    """
    # Twitter APIèªè¨¼
    client = tweepy.Client(bearer_token='YOUR_BEARER_TOKEN')

    sentiments = []
    for keyword in keywords:
        # ãƒ„ã‚¤ãƒ¼ãƒˆæ¤œç´¢
        tweets = client.search_recent_tweets(
            query=keyword,
            max_results=100,
            tweet_fields=['created_at', 'public_metrics']
        )

        for tweet in tweets.data:
            # æ„Ÿæƒ…åˆ†æ
            analysis = TextBlob(tweet.text)
            sentiment = analysis.sentiment.polarity  # -1ï¼ˆãƒã‚¬ãƒ†ã‚£ãƒ–ï¼‰ ~ 1ï¼ˆãƒã‚¸ãƒ†ã‚£ãƒ–ï¼‰
            sentiments.append({
                'date': tweet.created_at.date(),
                'keyword': keyword,
                'sentiment': sentiment
            })

    df_sentiment = pd.DataFrame(sentiments)

    # æ—¥æ¬¡é›†è¨ˆ
    daily_sentiment = df_sentiment.groupby('date')['sentiment'].mean()

    return daily_sentiment

# ä½¿ç”¨ä¾‹
# daily_sentiment = fetch_forex_sentiment(api_key, api_secret)
```

---

### ãƒ‹ãƒ¥ãƒ¼ã‚¹æ„Ÿæƒ…åˆ†æï¼ˆFinBERTï¼‰

```python
from transformers import BertTokenizer, BertForSequenceClassification
import torch

def analyze_news_sentiment(news_text):
    """
    FinBERTã§é‡‘èãƒ‹ãƒ¥ãƒ¼ã‚¹ã®æ„Ÿæƒ…åˆ†æ

    FinBERT: é‡‘èãƒ‹ãƒ¥ãƒ¼ã‚¹ç‰¹åŒ–ã®BERTãƒ¢ãƒ‡ãƒ«
    """
    # FinBERTãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
    tokenizer = BertTokenizer.from_pretrained('ProsusAI/finbert')
    model = BertForSequenceClassification.from_pretrained('ProsusAI/finbert')

    # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
    inputs = tokenizer(news_text, return_tensors='pt', truncation=True, max_length=512)

    # äºˆæ¸¬
    with torch.no_grad():
        outputs = model(**inputs)
        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)

    # ãƒ©ãƒ™ãƒ«: 0=negative, 1=neutral, 2=positive
    sentiment_score = predictions[0][2].item() - predictions[0][0].item()  # positive - negative

    return sentiment_score

# ä½¿ç”¨ä¾‹
news = "ç±³FRBã€åˆ©ä¸Šã’ãƒšãƒ¼ã‚¹åŠ é€Ÿã‚’ç¤ºå”†ã€‚ãƒ‰ãƒ«å††ã¯æ€¥é¨°ã—ã€150å††å°ã«åˆ°é”ã€‚"
sentiment = analyze_news_sentiment(news)
print(f"ãƒ‹ãƒ¥ãƒ¼ã‚¹æ„Ÿæƒ…ã‚¹ã‚³ã‚¢: {sentiment:.3f}")
```

---

## ğŸ“Š å®Ÿè£…å„ªå…ˆé †ä½ã¨æœŸå¾…åŠ¹æœã¾ã¨ã‚

### ãƒ•ã‚§ãƒ¼ã‚º1: å³åº§ã«å®Ÿè£…å¯èƒ½ï¼ˆ1é€±é–“ä»¥å†…ï¼‰

```
â˜ 1. ãƒ©ãƒ™ãƒ«å®šç¾©ã®è¦‹ç›´ã—ï¼ˆé–¾å€¤0.5%ï¼‰
   æœŸå¾…åŠ¹æœ: +5ã€œ10%
   å®Ÿè£…é›£æ˜“åº¦: ä½

â˜ 2. ä¿¡é ¼åº¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆé–¾å€¤0.65ï¼‰
   æœŸå¾…åŠ¹æœ: +5ã€œ8%
   å®Ÿè£…é›£æ˜“åº¦: ä½

â˜ 3. ãƒ‡ãƒ¼ã‚¿é‡æ‹¡å¼µï¼ˆ10å¹´åˆ†ï¼‰
   æœŸå¾…åŠ¹æœ: +3ã€œ5%
   å®Ÿè£…é›£æ˜“åº¦: ä½
```

**æœŸå¾…ã•ã‚Œã‚‹åˆè¨ˆæ”¹å–„**: +13ã€œ23%
**ç›®æ¨™ç²¾åº¦**: 79.34% + 15% = **94.34%**ï¼ˆç†è«–ä¸Šé™ã«åˆ°é”ï¼‰

---

### ãƒ•ã‚§ãƒ¼ã‚º2: ä¸­æœŸå®Ÿè£…ï¼ˆ2-3é€±é–“ï¼‰

```
â˜ 4. COTãƒ¬ãƒãƒ¼ãƒˆè¿½åŠ 
   æœŸå¾…åŠ¹æœ: +2ã€œ4%
   å®Ÿè£…é›£æ˜“åº¦: ä¸­

â˜ 5. ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
   æœŸå¾…åŠ¹æœ: +2ã€œ4%
   å®Ÿè£…é›£æ˜“åº¦: ä¸­

â˜ 6. æ™‚é–“é‡ã¿ä»˜ã‘ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
   æœŸå¾…åŠ¹æœ: +1ã€œ2%
   å®Ÿè£…é›£æ˜“åº¦: ä½
```

---

### ãƒ•ã‚§ãƒ¼ã‚º3: é•·æœŸå®Ÿè£…ï¼ˆ1-2ãƒ¶æœˆï¼‰

```
â˜ 7. ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æï¼ˆTwitter/ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼‰
   æœŸå¾…åŠ¹æœ: +3ã€œ5%
   å®Ÿè£…é›£æ˜“åº¦: é«˜

â˜ 8. LSTMè¿½åŠ 
   æœŸå¾…åŠ¹æœ: +1ã€œ3%
   å®Ÿè£…é›£æ˜“åº¦: é«˜
   å‰ææ¡ä»¶: 10å¹´åˆ†ãƒ‡ãƒ¼ã‚¿å¿…é ˆ
```

---

## ğŸš€ æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³

### ä»Šã™ãå®Ÿè£…ã™ã¹ãTop 3

1. **ãƒ©ãƒ™ãƒ«å®šç¾©ã®è¦‹ç›´ã—**
   - `src/model_builder/phase1_7_threshold_labels.py` ã‚’ä½œæˆ
   - é–¾å€¤0.5%ã§å®Ÿè£…
   - æœŸå¾…ç²¾åº¦: 84-89%

2. **ãƒ‡ãƒ¼ã‚¿é‡æ‹¡å¼µ**
   - Yahoo Financeã‹ã‚‰10å¹´åˆ†å–å¾—
   - æ™‚é–“é‡ã¿ä»˜ã‘ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Ÿè£…
   - æœŸå¾…ç²¾åº¦: 82-84%

3. **ä¿¡é ¼åº¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°**
   - é–¾å€¤0.65ã§å®Ÿè£…
   - ç²¾åº¦å„ªå…ˆæˆ¦ç•¥
   - æœŸå¾…ç²¾åº¦: 84-87%

### å®Ÿè£…ã‚³ãƒ¼ãƒ‰ä¾‹ï¼ˆçµ±åˆç‰ˆï¼‰

æ¬¡ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã§ã€ã“ã‚Œã‚‰3ã¤ã‚’çµ±åˆã—ãŸ **Phase 1.7 Enhanced** ã®å®Ÿè£…ã‚³ãƒ¼ãƒ‰ã‚’æä¾›ã—ã¾ã™ã€‚

---

**ä½œæˆæ—¥**: 2026-01-01
**å¯¾è±¡**: Phase 1ç²¾åº¦æ”¹å–„
**ç›®æ¨™**: 79.34% â†’ 85-90%
