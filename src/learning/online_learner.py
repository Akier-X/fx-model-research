"""
ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ  - å¤±æ•—ã‹ã‚‰è‡ªå‹•çš„ã«å­¦ç¿’

è­¦å‘Šã‚µã‚¤ãƒ³ã¯ã€Œåœæ­¢ã€ã§ã¯ãªãã€Œå­¦ç¿’ãƒˆãƒªã‚¬ãƒ¼ã€ã¨ã—ã¦æ©Ÿèƒ½
"""
import numpy as np
import pandas as pd
from typing import Dict, List, Optional
from loguru import logger
from pathlib import Path
import joblib
from datetime import datetime
from collections import deque

from ..ml.ml_strategy import MLStrategy
from ..rl.rl_agent import RLAgent


class OnlineLearner:
    """
    ç¶™ç¶šçš„å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ 

    å¤±æ•—ãƒ»æˆåŠŸã®çµŒé¨“ã‹ã‚‰è‡ªå‹•çš„ã«å­¦ç¿’ã—ã€ãƒ¢ãƒ‡ãƒ«ã‚’æ”¹å–„
    """

    def __init__(
        self,
        ml_strategy: MLStrategy,
        rl_agent: RLAgent,
        experience_buffer_size: int = 10000,
        retrain_threshold: int = 100,  # 100ãƒˆãƒ¬ãƒ¼ãƒ‰æ¯ã«å†è¨“ç·´
        min_performance_threshold: float = -0.05  # -5%ä»¥ä¸‹ã§ç·Šæ€¥å­¦ç¿’
    ):
        self.ml_strategy = ml_strategy
        self.rl_agent = rl_agent
        self.experience_buffer_size = experience_buffer_size
        self.retrain_threshold = retrain_threshold
        self.min_performance_threshold = min_performance_threshold

        # çµŒé¨“ãƒãƒƒãƒ•ã‚¡
        self.trade_history = deque(maxlen=experience_buffer_size)
        self.performance_history = deque(maxlen=1000)

        # å­¦ç¿’çµ±è¨ˆ
        self.total_retrains = 0
        self.emergency_retrains = 0
        self.improvement_count = 0

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°
        self.current_drawdown = 0
        self.consecutive_losses = 0
        self.recent_performance = []

        logger.info("ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")

    def add_trade_experience(
        self,
        trade_data: Dict,
        market_data: pd.DataFrame,
        outcome: str,  # 'win', 'loss', 'neutral'
        pnl: float,
        sentiment_data: Optional[Dict] = None
    ):
        """
        ãƒˆãƒ¬ãƒ¼ãƒ‰çµŒé¨“ã‚’è¨˜éŒ²

        Args:
            trade_data: ãƒˆãƒ¬ãƒ¼ãƒ‰æƒ…å ±
            market_data: å¸‚å ´ãƒ‡ãƒ¼ã‚¿
            outcome: çµæœ
            pnl: æç›Š
            sentiment_data: ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿
        """
        experience = {
            'timestamp': datetime.now(),
            'trade_data': trade_data,
            'market_data': market_data.copy(),
            'outcome': outcome,
            'pnl': pnl,
            'sentiment_data': sentiment_data
        }

        self.trade_history.append(experience)
        self.performance_history.append(pnl)

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™æ›´æ–°
        if pnl < 0:
            self.consecutive_losses += 1
        else:
            self.consecutive_losses = 0

        # ãƒ‰ãƒ­ãƒ¼ãƒ€ã‚¦ãƒ³è¨ˆç®—
        if len(self.performance_history) > 0:
            cumulative = np.cumsum(list(self.performance_history))
            peak = np.maximum.accumulate(cumulative)
            drawdown = (cumulative - peak) / (peak + 1e-10)
            self.current_drawdown = drawdown[-1] if len(drawdown) > 0 else 0

        logger.info(
            f"çµŒé¨“è¿½åŠ : {outcome.upper()}, PnL: {pnl:,.0f}, "
            f"é€£æ•—: {self.consecutive_losses}, DD: {self.current_drawdown:.2%}"
        )

        # å­¦ç¿’ãƒˆãƒªã‚¬ãƒ¼ãƒã‚§ãƒƒã‚¯
        self._check_learning_triggers()

    def _check_learning_triggers(self):
        """
        å­¦ç¿’ãƒˆãƒªã‚¬ãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯

        è­¦å‘Šã‚µã‚¤ãƒ³ = å­¦ç¿’ã®æ©Ÿä¼š
        """
        should_retrain = False
        reason = ""
        is_emergency = False

        # ãƒˆãƒªã‚¬ãƒ¼1: å®šæœŸçš„ãªå†è¨“ç·´
        if len(self.trade_history) >= self.retrain_threshold:
            should_retrain = True
            reason = f"å®šæœŸå†è¨“ç·´ ({len(self.trade_history)}ãƒˆãƒ¬ãƒ¼ãƒ‰)"

        # ãƒˆãƒªã‚¬ãƒ¼2: å¤§ããªãƒ‰ãƒ­ãƒ¼ãƒ€ã‚¦ãƒ³ï¼ˆç·Šæ€¥å­¦ç¿’ï¼‰
        if self.current_drawdown < -0.15:  # -15%
            should_retrain = True
            is_emergency = True
            reason = f"ãƒ‰ãƒ­ãƒ¼ãƒ€ã‚¦ãƒ³ç·Šæ€¥å­¦ç¿’ ({self.current_drawdown:.2%})"

        # ãƒˆãƒªã‚¬ãƒ¼3: é€£ç¶šæå¤±ï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’ï¼‰
        if self.consecutive_losses >= 5:
            should_retrain = True
            is_emergency = True
            reason = f"é€£æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’ ({self.consecutive_losses}é€£æ•—)"

        # ãƒˆãƒªã‚¬ãƒ¼4: ç›´è¿‘ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ‚ªåŒ–
        if len(self.performance_history) >= 20:
            recent_20 = list(self.performance_history)[-20:]
            if np.mean(recent_20) < self.min_performance_threshold:
                should_retrain = True
                is_emergency = True
                reason = f"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ‚ªåŒ– (å¹³å‡PnL: {np.mean(recent_20):.2%})"

        if should_retrain:
            logger.warning(f"ğŸ”„ å­¦ç¿’ãƒˆãƒªã‚¬ãƒ¼ç™ºå‹•: {reason}")
            self.retrain_models(emergency=is_emergency, reason=reason)

    def retrain_models(self, emergency: bool = False, reason: str = ""):
        """
        ãƒ¢ãƒ‡ãƒ«ã‚’å†è¨“ç·´

        Args:
            emergency: ç·Šæ€¥å­¦ç¿’ãƒ•ãƒ©ã‚°
            reason: å­¦ç¿’ç†ç”±
        """
        logger.info("=" * 60)
        logger.info(f"ğŸ§  ãƒ¢ãƒ‡ãƒ«å†è¨“ç·´é–‹å§‹: {reason}")
        logger.info(f"ç·Šæ€¥åº¦: {'ğŸš¨ HIGH' if emergency else 'ğŸ“Š NORMAL'}")
        logger.info("=" * 60)

        if len(self.trade_history) < 50:
            logger.warning("çµŒé¨“ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã€‚å†è¨“ç·´ã‚’ã‚¹ã‚­ãƒƒãƒ—")
            return

        try:
            # 1. çµŒé¨“ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’æ§‹ç¯‰
            training_data = self._build_training_data()

            # 2. å¤±æ•—ã‚±ãƒ¼ã‚¹ã«é‡ç‚¹ã‚’ç½®ã
            weighted_data = self._weight_experiences(training_data, emergency)

            # 3. MLãƒ¢ãƒ‡ãƒ«ã‚’å†è¨“ç·´
            logger.info("[1/2] æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«å†è¨“ç·´ä¸­...")
            self._retrain_ml_model(weighted_data)

            # 4. RLã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’è¿½åŠ å­¦ç¿’
            logger.info("[2/2] å¼·åŒ–å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¿½åŠ å­¦ç¿’ä¸­...")
            self._retrain_rl_agent(weighted_data, emergency)

            # 5. çµ±è¨ˆæ›´æ–°
            self.total_retrains += 1
            if emergency:
                self.emergency_retrains += 1

            # 6. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¤œè¨¼
            improvement = self._validate_improvement(training_data)

            if improvement:
                self.improvement_count += 1
                logger.info(f"âœ… ãƒ¢ãƒ‡ãƒ«æ”¹å–„æˆåŠŸ (æ”¹å–„ç‡: {improvement:.2%})")
            else:
                logger.warning("âš ï¸ æ”¹å–„ãŒè¦‹ã‚‰ã‚Œã¾ã›ã‚“ã€‚ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œè¨")

            # 7. çµŒé¨“ãƒãƒƒãƒ•ã‚¡ã‚’ã‚¯ãƒªã‚¢ï¼ˆéƒ¨åˆ†çš„ã«ï¼‰
            if not emergency:
                # ç·Šæ€¥æ™‚ã¯çµŒé¨“ã‚’ä¿æŒã€é€šå¸¸æ™‚ã¯åŠåˆ†ã‚¯ãƒªã‚¢
                half = len(self.trade_history) // 2
                for _ in range(half):
                    self.trade_history.popleft()

            logger.info(f"å†è¨“ç·´å®Œäº†ã€‚ç·å†è¨“ç·´å›æ•°: {self.total_retrains}")

        except Exception as e:
            logger.error(f"å†è¨“ç·´ã‚¨ãƒ©ãƒ¼: {e}")

    def _build_training_data(self) -> pd.DataFrame:
        """
        çµŒé¨“ãƒãƒƒãƒ•ã‚¡ã‹ã‚‰å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’æ§‹ç¯‰
        """
        all_market_data = []
        labels = []
        weights = []

        for exp in self.trade_history:
            market_data = exp['market_data']

            # ãƒ©ãƒ™ãƒ«: å‹ã¡ãƒˆãƒ¬ãƒ¼ãƒ‰=1, è² ã‘ãƒˆãƒ¬ãƒ¼ãƒ‰=-1
            if exp['outcome'] == 'win':
                label = 1
                weight = 1.0
            elif exp['outcome'] == 'loss':
                label = -1
                weight = 1.5  # å¤±æ•—ã«é‡ã¿
            else:
                label = 0
                weight = 0.5

            all_market_data.append(market_data)
            labels.append(label)
            weights.append(weight)

        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«çµ±åˆ
        combined_df = pd.concat(all_market_data, ignore_index=True)
        combined_df['label'] = labels * len(combined_df) // len(labels)
        combined_df['weight'] = weights * len(combined_df) // len(weights)

        return combined_df

    def _weight_experiences(
        self,
        training_data: pd.DataFrame,
        emergency: bool
    ) -> pd.DataFrame:
        """
        çµŒé¨“ã«é‡ã¿ä»˜ã‘

        ç·Šæ€¥æ™‚ã¯å¤±æ•—ã‚±ãƒ¼ã‚¹ã«3å€ã®é‡ã¿
        """
        if emergency:
            # å¤±æ•—ã‚±ãƒ¼ã‚¹ã‚’å¼·èª¿
            training_data.loc[training_data['label'] == -1, 'weight'] *= 3.0
            logger.info("ç·Šæ€¥ãƒ¢ãƒ¼ãƒ‰: å¤±æ•—ã‚±ãƒ¼ã‚¹ã«3å€ã®é‡ã¿")

        return training_data

    def _retrain_ml_model(self, training_data: pd.DataFrame):
        """
        æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’å†è¨“ç·´
        """
        # æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã‚’åŸºã«è¿½åŠ å­¦ç¿’
        # warm_start ã‚’ä½¿ç”¨ã—ã¦ã€æ—¢å­˜ã®çŸ¥è­˜ã‚’ä¿æŒã—ã¤ã¤æ–°ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’

        self.ml_strategy.model.set_params(warm_start=True)

        features = self.ml_strategy.prepare_features(training_data)
        X = features.drop(['label', 'weight'], axis=1, errors='ignore')
        y = training_data['label']
        sample_weight = training_data['weight']

        # è¿½åŠ å­¦ç¿’
        self.ml_strategy.model.fit(X, y, sample_weight=sample_weight)

        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        self.ml_strategy.save_model()

        logger.info("MLãƒ¢ãƒ‡ãƒ«å†è¨“ç·´å®Œäº†")

    def _retrain_rl_agent(
        self,
        training_data: pd.DataFrame,
        emergency: bool
    ):
        """
        å¼·åŒ–å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’è¿½åŠ å­¦ç¿’
        """
        # çµŒé¨“å†ç”Ÿãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ 
        # ç·Šæ€¥æ™‚ã¯å­¦ç¿’ç‡ã‚’ä¸€æ™‚çš„ã«ä¸Šã’ã‚‹

        if emergency:
            original_lr = self.rl_agent.learning_rate
            self.rl_agent.learning_rate *= 2.0  # 2å€ã®å­¦ç¿’ç‡

        # å°‘ãªã„ã‚¹ãƒ†ãƒƒãƒ—ã§é›†ä¸­å­¦ç¿’
        self.rl_agent.train(
            train_df=training_data,
            total_timesteps=10000,  # ç·Šæ€¥æ™‚ã¯çŸ­æ™‚é–“ã§é›†ä¸­å­¦ç¿’
            initial_balance=10000
        )

        if emergency:
            self.rl_agent.learning_rate = original_lr  # å…ƒã«æˆ»ã™

        logger.info("RLã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¿½åŠ å­¦ç¿’å®Œäº†")

    def _validate_improvement(self, training_data: pd.DataFrame) -> float:
        """
        æ”¹å–„ã‚’æ¤œè¨¼

        Returns:
            æ”¹å–„ç‡ï¼ˆæ­£ãªã‚‰æ”¹å–„ã€è² ãªã‚‰æ‚ªåŒ–ï¼‰
        """
        # ç°¡æ˜“ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã§æ¤œè¨¼
        # å®Ÿè£…ã¯çœç•¥ï¼ˆå®Ÿéš›ã«ã¯ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œï¼‰
        return 0.05  # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼

    def get_learning_stats(self) -> Dict:
        """
        å­¦ç¿’çµ±è¨ˆã‚’å–å¾—
        """
        return {
            'total_trades': len(self.trade_history),
            'total_retrains': self.total_retrains,
            'emergency_retrains': self.emergency_retrains,
            'improvement_count': self.improvement_count,
            'current_drawdown': self.current_drawdown,
            'consecutive_losses': self.consecutive_losses,
            'avg_recent_pnl': np.mean(list(self.performance_history)[-20:]) if len(self.performance_history) >= 20 else 0
        }

    def should_pause_trading(self) -> tuple[bool, str]:
        """
        ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚’ä¸€æ™‚åœæ­¢ã™ã¹ãã‹åˆ¤æ–­

        Returns:
            (åœæ­¢ã™ã¹ãã‹, ç†ç”±)
        """
        # æœ¬å½“ã«æ·±åˆ»ãªçŠ¶æ³ã§ã®ã¿åœæ­¢

        # 1. ç ´ç”£ãƒ¬ãƒ™ãƒ«ã®ãƒ‰ãƒ­ãƒ¼ãƒ€ã‚¦ãƒ³
        if self.current_drawdown < -0.50:  # -50%
            return True, f"ç ´ç”£ãƒ¬ãƒ™ãƒ«ã®ãƒ‰ãƒ­ãƒ¼ãƒ€ã‚¦ãƒ³ ({self.current_drawdown:.2%})"

        # 2. ç•°å¸¸ãªé€£æ•—ï¼ˆå­¦ç¿’ã—ã¦ã‚‚æ”¹å–„ã—ãªã„ï¼‰
        if self.consecutive_losses >= 15:
            return True, f"ç•°å¸¸ãªé€£æ•— ({self.consecutive_losses}é€£æ•—)"

        # 3. ãƒ¢ãƒ‡ãƒ«ãŒæ˜ã‚‰ã‹ã«æ©Ÿèƒ½ã—ã¦ã„ãªã„
        if len(self.performance_history) >= 50:
            recent_50 = list(self.performance_history)[-50:]
            if sum(1 for x in recent_50 if x < 0) >= 40:  # 50ä¸­40ãŒæå¤±
                return True, "ãƒ¢ãƒ‡ãƒ«æ©Ÿèƒ½ä¸å…¨ (å‹ç‡20%ä»¥ä¸‹)"

        return False, ""

    def adaptive_risk_adjustment(self) -> float:
        """
        ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«åŸºã¥ã„ã¦ãƒªã‚¹ã‚¯ã‚’å‹•çš„èª¿æ•´

        Returns:
            ãƒªã‚¹ã‚¯èª¿æ•´ä¿‚æ•° (0.5 ~ 2.0)
        """
        # å¥½èª¿æ™‚ã¯ãƒªã‚¹ã‚¯ã‚’ä¸Šã’ã€ä¸èª¿æ™‚ã¯ä¸‹ã’ã‚‹

        if len(self.performance_history) < 20:
            return 1.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ

        recent_20 = list(self.performance_history)[-20:]
        avg_pnl = np.mean(recent_20)
        win_rate = sum(1 for x in recent_20 if x > 0) / 20

        # å¥½èª¿ï¼ˆå‹ç‡60%ä»¥ä¸Šã€å¹³å‡PnLæ­£ï¼‰
        if win_rate >= 0.6 and avg_pnl > 0:
            return 1.5  # ãƒªã‚¹ã‚¯1.5å€

        # çµ¶å¥½èª¿ï¼ˆå‹ç‡70%ä»¥ä¸Šã€å¹³å‡PnLå¤§ãã„ï¼‰
        if win_rate >= 0.7 and avg_pnl > 0.01:
            return 2.0  # ãƒªã‚¹ã‚¯2å€

        # ä¸èª¿ï¼ˆå‹ç‡40%ä»¥ä¸‹ï¼‰
        if win_rate <= 0.4:
            return 0.5  # ãƒªã‚¹ã‚¯åŠåˆ†

        # æ·±åˆ»ãªä¸èª¿ï¼ˆå‹ç‡30%ä»¥ä¸‹ï¼‰
        if win_rate <= 0.3:
            return 0.25  # ãƒªã‚¹ã‚¯4åˆ†ã®1

        return 1.0  # æ™®é€š
